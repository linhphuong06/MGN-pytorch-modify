{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linhphuong/anaconda3/envs/id/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/linhphuong/anaconda3/envs/id/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MGN(\n",
      "  (backone): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (p1): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (p2): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (p3): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (maxpool_zg_p1): AvgPool2d(kernel_size=(12, 4), stride=(12, 4), padding=0)\n",
      "  (maxpool_zg_p2): AvgPool2d(kernel_size=(24, 8), stride=(24, 8), padding=0)\n",
      "  (maxpool_zg_p3): AvgPool2d(kernel_size=(24, 8), stride=(24, 8), padding=0)\n",
      "  (maxpool_zp2): AvgPool2d(kernel_size=(12, 8), stride=(12, 8), padding=0)\n",
      "  (maxpool_zp3): AvgPool2d(kernel_size=(8, 8), stride=(8, 8), padding=0)\n",
      "  (reduction_0): Sequential(\n",
      "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (reduction_1): Sequential(\n",
      "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (reduction_2): Sequential(\n",
      "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (reduction_3): Sequential(\n",
      "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (reduction_4): Sequential(\n",
      "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (reduction_5): Sequential(\n",
      "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (reduction_6): Sequential(\n",
      "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (reduction_7): Sequential(\n",
      "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc_id_2048_0): Linear(in_features=256, out_features=751, bias=True)\n",
      "  (fc_id_2048_1): Linear(in_features=256, out_features=751, bias=True)\n",
      "  (fc_id_2048_2): Linear(in_features=256, out_features=751, bias=True)\n",
      "  (fc_id_256_1_0): Linear(in_features=256, out_features=751, bias=True)\n",
      "  (fc_id_256_1_1): Linear(in_features=256, out_features=751, bias=True)\n",
      "  (fc_id_256_2_0): Linear(in_features=256, out_features=751, bias=True)\n",
      "  (fc_id_256_2_1): Linear(in_features=256, out_features=751, bias=True)\n",
      "  (fc_id_256_2_2): Linear(in_features=256, out_features=751, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MGN(\n",
       "  (backone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (p1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (p2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (p3): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (maxpool_zg_p1): AvgPool2d(kernel_size=(12, 4), stride=(12, 4), padding=0)\n",
       "  (maxpool_zg_p2): AvgPool2d(kernel_size=(24, 8), stride=(24, 8), padding=0)\n",
       "  (maxpool_zg_p3): AvgPool2d(kernel_size=(24, 8), stride=(24, 8), padding=0)\n",
       "  (maxpool_zp2): AvgPool2d(kernel_size=(12, 8), stride=(12, 8), padding=0)\n",
       "  (maxpool_zp3): AvgPool2d(kernel_size=(8, 8), stride=(8, 8), padding=0)\n",
       "  (reduction_0): Sequential(\n",
       "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (reduction_1): Sequential(\n",
       "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (reduction_2): Sequential(\n",
       "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (reduction_3): Sequential(\n",
       "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (reduction_4): Sequential(\n",
       "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (reduction_5): Sequential(\n",
       "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (reduction_6): Sequential(\n",
       "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (reduction_7): Sequential(\n",
       "    (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc_id_2048_0): Linear(in_features=256, out_features=751, bias=True)\n",
       "  (fc_id_2048_1): Linear(in_features=256, out_features=751, bias=True)\n",
       "  (fc_id_2048_2): Linear(in_features=256, out_features=751, bias=True)\n",
       "  (fc_id_256_1_0): Linear(in_features=256, out_features=751, bias=True)\n",
       "  (fc_id_256_1_1): Linear(in_features=256, out_features=751, bias=True)\n",
       "  (fc_id_256_2_0): Linear(in_features=256, out_features=751, bias=True)\n",
       "  (fc_id_256_2_1): Linear(in_features=256, out_features=751, bias=True)\n",
       "  (fc_id_256_2_2): Linear(in_features=256, out_features=751, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import mgn\n",
    "import torch\n",
    "import yaml\n",
    "# nh ngha cc tham s ca model\n",
    "# args =  config_str\n",
    "  # Tham s ca model (nu c)\n",
    "\n",
    "# Khi to model\n",
    "# num_classes = 751\n",
    "# config_file = '/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/config.yml'\n",
    "# with open(config_file, 'r') as cf:\n",
    "#     config_str = \"\\n\" + cf.read()\n",
    "\n",
    "# args = yaml.safe_load(config_str)\n",
    "# args = config_str\n",
    "# num_classes = args['num_classes']\n",
    "# model = mgn.MGN(args)\n",
    "class ConfigObject:\n",
    "    pass\n",
    "\n",
    "# ng dn n tp YAML\n",
    "yaml_file_path = '/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/config.yml'\n",
    "\n",
    "# c tp YAML\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# To mt i tng t d liu YAML\n",
    "config = ConfigObject()\n",
    "for key, value in yaml_data.items():\n",
    "    setattr(config, key, value)\n",
    "model = mgn.MGN(config)\n",
    "print(model)\n",
    "# ng dn n file cha trng s  lu\n",
    "path_to_weights = '/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/model_2.pth'\n",
    "\n",
    "model_load = torch.load(path_to_weights)\n",
    "for key in list(model_load.keys()):\n",
    "    new_key = key.replace('model.', '')\n",
    "    model_load[new_key] = model_load.pop(key)\n",
    "# Ti trng s  lu\n",
    "model.load_state_dict(model_load)\n",
    "\n",
    "# Chuyn model sang ch  nh gi (evaluation mode)\n",
    "model.eval()\n",
    "\n",
    "# S dng model  thc hin d on trn d liu mi (nu cn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils.utility as utility\n",
    "from scipy.spatial.distance import cdist\n",
    "from utils.functions import cmc, mean_ap\n",
    "from utils.re_ranking import re_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "def extract_transforms(input_image):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((384,128), interpolation=3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "image_path = \"/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/data/Market-1501-v15.09.15/query/0005_c2s1_000976_00.jpg\"\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = extract_transforms(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7722, 0.4122, 1.3220,  ..., 0.5726, 0.0479, 0.5804]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " tensor([[0.7722, 0.4122, 1.3220, 0.0000, 0.0000, 0.6904, 0.4226, 0.0000, 0.3096,\n",
       "          0.0000, 0.0681, 0.5600, 0.0000, 0.4786, 0.0403, 0.2694, 0.0000, 0.0000,\n",
       "          0.0000, 0.5123, 0.4989, 0.0000, 0.0000, 0.0000, 2.5041, 0.0000, 1.3342,\n",
       "          0.8394, 1.4132, 0.5977, 0.0000, 0.0000, 0.0000, 1.7530, 0.0000, 0.0000,\n",
       "          0.0000, 0.0164, 0.0000, 0.7505, 0.6525, 0.0000, 0.0000, 0.0000, 1.1747,\n",
       "          0.0000, 0.0000, 0.8754, 1.1577, 0.0000, 0.0000, 0.0000, 0.8064, 0.3281,\n",
       "          0.1120, 0.0000, 1.6208, 0.6309, 0.0461, 0.7399, 0.0000, 0.0000, 1.1718,\n",
       "          0.0000, 0.2031, 0.0000, 0.0000, 0.1111, 0.0000, 1.3140, 0.0000, 0.0000,\n",
       "          0.2820, 0.4093, 0.0000, 0.0000, 0.2870, 1.6629, 1.7339, 0.0000, 0.0000,\n",
       "          0.0000, 1.2726, 0.2359, 0.1444, 0.0502, 1.9064, 0.0000, 0.7665, 0.4008,\n",
       "          0.3255, 0.0000, 0.2079, 0.0000, 0.0000, 0.0000, 0.9305, 0.0000, 1.3412,\n",
       "          0.7048, 0.0000, 0.1026, 0.0085, 0.0000, 0.0871, 0.2243, 0.6446, 1.5531,\n",
       "          0.0000, 1.4518, 0.0000, 0.0000, 0.8815, 0.0000, 0.0000, 1.0268, 0.2266,\n",
       "          0.0000, 0.0000, 0.0000, 0.8196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0705,\n",
       "          0.0000, 0.0000, 1.1677, 0.0000, 0.0000, 0.0000, 1.8189, 0.0000, 0.0000,\n",
       "          1.1534, 0.0000, 0.0000, 0.0000, 1.5236, 0.1024, 0.3659, 0.5388, 0.6574,\n",
       "          0.0000, 0.0000, 0.0000, 0.7584, 0.0000, 1.1039, 0.0000, 2.0014, 1.3698,\n",
       "          0.2306, 0.0000, 0.0000, 0.0000, 0.1853, 0.0000, 0.0000, 0.0000, 0.0373,\n",
       "          0.0619, 0.2997, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4671,\n",
       "          0.0000, 1.4625, 0.1860, 0.0000, 0.0000, 0.0000, 1.3122, 0.9682, 0.5545,\n",
       "          0.0000, 0.0000, 0.2086, 0.0000, 0.3531, 0.7958, 0.0000, 0.0000, 0.0000,\n",
       "          0.3387, 0.0000, 0.0037, 0.2903, 0.0000, 0.0000, 0.3218, 0.1757, 0.0000,\n",
       "          0.0000, 0.9770, 0.6335, 0.0000, 0.1478, 3.3713, 0.7720, 0.1643, 0.0000,\n",
       "          0.0000, 0.0766, 0.0000, 0.0000, 0.0310, 0.4041, 0.0000, 1.0206, 0.4279,\n",
       "          0.6768, 0.0000, 0.6126, 0.0000, 0.7336, 0.0000, 0.0000, 0.9583, 1.1032,\n",
       "          0.0000, 0.0000, 0.0000, 1.1630, 0.7961, 0.0000, 0.0786, 1.4293, 0.3138,\n",
       "          0.0000, 1.2599, 0.5979, 0.6855, 0.6156, 0.0000, 0.0000, 0.0000, 0.9216,\n",
       "          0.0000, 0.2354, 0.0000, 0.0498, 0.1748, 0.0296, 0.0000, 0.0000, 0.4926,\n",
       "          0.0000, 0.6349, 0.0000, 0.7011]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.3899, 0.0065, 1.2652, 0.0000, 0.2184, 0.3919, 0.5413, 0.0000, 0.5207,\n",
       "          0.0000, 0.3101, 0.3970, 0.0000, 0.3389, 0.1664, 0.2217, 0.0000, 0.0000,\n",
       "          0.0000, 0.3270, 0.4169, 0.0000, 0.0000, 0.0000, 1.6840, 0.0000, 1.3302,\n",
       "          0.9996, 1.2428, 0.7302, 0.0000, 0.0065, 0.0000, 1.2356, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.2306, 0.5204, 0.0000, 0.0000, 0.0000, 1.1121,\n",
       "          0.0000, 0.0000, 0.5773, 0.7965, 0.0000, 0.0000, 0.0000, 0.5415, 0.0000,\n",
       "          0.1469, 0.0000, 0.8112, 0.3675, 0.0177, 0.7402, 0.0000, 0.0000, 0.6068,\n",
       "          0.0000, 0.0432, 0.0000, 0.0000, 0.0000, 0.0000, 1.2757, 0.0000, 0.0000,\n",
       "          0.0421, 0.4866, 0.0000, 0.0000, 0.2982, 1.2369, 1.4171, 0.0000, 0.0000,\n",
       "          0.0000, 1.0990, 0.0000, 0.2572, 0.4640, 1.6862, 0.0000, 0.7565, 0.1292,\n",
       "          0.4396, 0.0000, 0.1980, 0.0000, 0.0359, 0.0000, 0.4955, 0.0000, 0.7171,\n",
       "          0.4789, 0.0000, 0.1112, 0.0402, 0.0000, 0.0000, 0.0000, 0.6452, 1.0610,\n",
       "          0.0000, 1.5585, 0.0000, 0.0000, 0.4189, 0.0000, 0.0000, 1.0197, 0.3369,\n",
       "          0.0000, 0.0000, 0.0000, 0.9165, 0.0503, 0.0000, 0.0000, 0.0000, 0.1961,\n",
       "          0.0000, 0.0000, 0.7707, 0.0000, 0.0000, 0.0000, 1.1396, 0.0000, 0.0000,\n",
       "          0.8527, 0.0000, 0.0000, 0.0000, 0.9638, 0.4677, 0.3227, 0.1713, 0.4369,\n",
       "          0.0000, 0.0000, 0.0000, 0.6661, 0.1780, 0.6031, 0.0000, 1.3320, 1.2695,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0838, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.2070, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5685,\n",
       "          0.0000, 1.0151, 0.4453, 0.0000, 0.0000, 0.0000, 0.9448, 0.7697, 0.6127,\n",
       "          0.0000, 0.0000, 0.1341, 0.0000, 0.0000, 0.6339, 0.0000, 0.0000, 0.0000,\n",
       "          0.2737, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6050, 0.0199, 0.0718,\n",
       "          0.0000, 0.4086, 0.0705, 0.0000, 0.1700, 2.4058, 0.6310, 0.1302, 0.0149,\n",
       "          0.0000, 0.2897, 0.0000, 0.0000, 0.3056, 0.1619, 0.0000, 1.0261, 0.0000,\n",
       "          0.2474, 0.0000, 0.3862, 0.0000, 0.4228, 0.0540, 0.0000, 0.9121, 0.7602,\n",
       "          0.0000, 0.0000, 0.0000, 0.7146, 0.1193, 0.0000, 0.0000, 1.2138, 0.4615,\n",
       "          0.0000, 1.1505, 0.0600, 0.5620, 0.3663, 0.0000, 0.1748, 0.0000, 0.5204,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1505, 0.0000, 0.0000, 0.0000, 0.7176,\n",
       "          0.0000, 0.4477, 0.0000, 0.6036]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.3899, 0.0065, 1.2652, 0.0000, 0.2184, 0.3919, 0.5413, 0.0000, 0.5207,\n",
       "          0.0000, 0.3101, 0.3970, 0.0000, 0.3389, 0.1664, 0.2217, 0.0000, 0.0000,\n",
       "          0.0000, 0.3270, 0.4169, 0.0000, 0.0000, 0.0000, 1.6840, 0.0000, 1.3302,\n",
       "          0.9996, 1.2428, 0.7302, 0.0000, 0.0065, 0.0000, 1.2356, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.2306, 0.5204, 0.0000, 0.0000, 0.0000, 1.1121,\n",
       "          0.0000, 0.0000, 0.5773, 0.7965, 0.0000, 0.0000, 0.0000, 0.5415, 0.0000,\n",
       "          0.1469, 0.0000, 0.8112, 0.3675, 0.0177, 0.7402, 0.0000, 0.0000, 0.6068,\n",
       "          0.0000, 0.0432, 0.0000, 0.0000, 0.0000, 0.0000, 1.2757, 0.0000, 0.0000,\n",
       "          0.0421, 0.4866, 0.0000, 0.0000, 0.2982, 1.2369, 1.4171, 0.0000, 0.0000,\n",
       "          0.0000, 1.0990, 0.0000, 0.2572, 0.4640, 1.6862, 0.0000, 0.7565, 0.1292,\n",
       "          0.4396, 0.0000, 0.1980, 0.0000, 0.0359, 0.0000, 0.4955, 0.0000, 0.7171,\n",
       "          0.4789, 0.0000, 0.1112, 0.0402, 0.0000, 0.0000, 0.0000, 0.6452, 1.0610,\n",
       "          0.0000, 1.5585, 0.0000, 0.0000, 0.4189, 0.0000, 0.0000, 1.0197, 0.3369,\n",
       "          0.0000, 0.0000, 0.0000, 0.9165, 0.0503, 0.0000, 0.0000, 0.0000, 0.1961,\n",
       "          0.0000, 0.0000, 0.7707, 0.0000, 0.0000, 0.0000, 1.1396, 0.0000, 0.0000,\n",
       "          0.8527, 0.0000, 0.0000, 0.0000, 0.9638, 0.4677, 0.3227, 0.1713, 0.4369,\n",
       "          0.0000, 0.0000, 0.0000, 0.6661, 0.1780, 0.6031, 0.0000, 1.3320, 1.2695,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0838, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.2070, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5685,\n",
       "          0.0000, 1.0151, 0.4453, 0.0000, 0.0000, 0.0000, 0.9448, 0.7697, 0.6127,\n",
       "          0.0000, 0.0000, 0.1341, 0.0000, 0.0000, 0.6339, 0.0000, 0.0000, 0.0000,\n",
       "          0.2737, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6050, 0.0199, 0.0718,\n",
       "          0.0000, 0.4086, 0.0705, 0.0000, 0.1700, 2.4058, 0.6310, 0.1302, 0.0149,\n",
       "          0.0000, 0.2897, 0.0000, 0.0000, 0.3056, 0.1619, 0.0000, 1.0261, 0.0000,\n",
       "          0.2474, 0.0000, 0.3862, 0.0000, 0.4228, 0.0540, 0.0000, 0.9121, 0.7602,\n",
       "          0.0000, 0.0000, 0.0000, 0.7146, 0.1193, 0.0000, 0.0000, 1.2138, 0.4615,\n",
       "          0.0000, 1.1505, 0.0600, 0.5620, 0.3663, 0.0000, 0.1748, 0.0000, 0.5204,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1505, 0.0000, 0.0000, 0.0000, 0.7176,\n",
       "          0.0000, 0.4477, 0.0000, 0.6036]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.4699, -0.3827,  1.1692, -0.6915, -0.3139, -0.1895,  0.9601,  0.2843,\n",
       "          -1.1100, -0.0192,  0.7512, -0.4854, -0.5360,  0.3450,  0.2599,  0.9795,\n",
       "           0.5662,  0.4739,  0.3211,  0.5650,  0.2962, -0.2231, -0.6840, -0.1864,\n",
       "          -0.0502, -0.3360, -0.2732, -0.3778, -0.4581, -0.0205, -0.2550,  0.2723,\n",
       "          -0.1028, -0.6966, -0.3828, -0.5639, -0.7382,  0.0350,  0.2633, -0.3108,\n",
       "           0.3734,  0.2617, -0.6401,  0.6937,  0.2086,  0.6791, -0.4949, -0.0902,\n",
       "           0.8353, -0.1647,  1.4315, -0.1415,  0.0923, -0.2894,  0.1895,  0.6702,\n",
       "          -0.3448,  0.6324, -0.1020, -0.1033,  0.2599,  0.2497,  0.6886, -0.6763,\n",
       "          -0.6133,  0.6273, -0.2714,  0.0425, -0.0025, -0.4149, -0.0280,  0.0116,\n",
       "          -0.3934, -0.3288,  0.2532, -0.1293,  0.0306,  0.6190,  0.0323, -0.9814,\n",
       "          -0.2193, -1.1484,  0.4248, -0.3519, -0.4583,  0.1464,  0.5742, -0.0310,\n",
       "          -0.0421, -0.2154,  0.5103,  0.4742, -0.1831,  0.1789, -0.2177, -0.3562,\n",
       "          -0.1857,  0.5555, -0.2850, -0.0160, -0.0612,  0.4707,  0.0727,  0.1398,\n",
       "          -0.1733,  0.4759,  0.5561, -0.9215,  0.5265,  0.4090, -0.3224,  0.6893,\n",
       "          -0.0441,  0.2282, -1.1453,  0.8704,  0.5327, -0.5142,  0.2754,  0.7841,\n",
       "          -0.0476, -0.6058,  0.8962, -0.5162, -1.0548, -0.1746,  0.3772, -1.0294,\n",
       "           0.1801,  0.1557,  0.0847,  0.7437, -0.4853,  0.4889, -0.6223, -0.4628,\n",
       "          -0.2270, -0.0517, -0.8443, -0.8424, -0.4985, -0.6796,  0.3249,  0.2604,\n",
       "           0.4609, -0.3110,  0.6286,  0.7586, -0.1580, -0.0656, -0.0077, -0.6097,\n",
       "           0.6585,  0.5073, -0.5094, -0.0877,  0.3068,  1.0243,  0.6928,  0.8991,\n",
       "          -0.6419, -0.1442,  0.5041, -0.4267, -0.0079,  0.7606, -0.5112, -0.0376,\n",
       "           0.6214,  0.3379, -0.1072,  0.8597,  0.0167, -0.4479, -0.4489, -0.4610,\n",
       "           0.3051, -0.1186,  1.0107, -0.0123,  0.4013,  0.2108, -0.0210, -1.0049,\n",
       "          -0.4694,  0.1721, -0.5135, -0.6694, -0.1576,  1.3060,  0.7299,  0.9641,\n",
       "           0.2901,  0.1648,  0.4476, -0.0451, -0.2926, -0.4137, -0.1205,  0.0767,\n",
       "          -1.2708, -0.9761,  0.4435, -0.9029,  0.0419, -0.1940, -0.0956, -0.3493,\n",
       "          -1.1991,  1.3857,  0.5577,  0.2443,  0.1711,  0.4746, -0.7229,  0.7147,\n",
       "           0.4324,  0.4528, -0.5426,  0.4540, -0.9327,  0.0629,  0.9624,  0.1531,\n",
       "          -0.4844, -0.4869,  0.0676,  0.5319,  0.0738,  0.4587,  0.0165, -0.0611,\n",
       "          -0.6114,  0.6031,  0.3572, -0.1075,  1.1619,  0.2397, -0.0110, -0.1313,\n",
       "           0.6856, -0.6198,  0.1609, -0.7952,  0.5853,  0.1831,  0.1016,  0.4787,\n",
       "          -0.4541,  0.4039,  0.3783, -0.2235, -0.5799,  0.4476, -1.1645, -0.5975,\n",
       "           0.5294, -0.5352,  0.2600, -0.3145,  1.0806, -0.5781, -0.0232,  0.5197,\n",
       "          -0.2281,  0.1584, -0.5920, -0.3250, -0.3854, -0.9998,  0.1337, -0.2129,\n",
       "          -0.5851, -0.3698,  0.5305,  0.1529, -0.4019, -0.8102, -0.5209, -0.1964,\n",
       "          -0.1154,  0.0393, -0.4549, -0.1676,  0.2583, -0.0207,  0.0150,  0.0475,\n",
       "           0.7637,  0.3355, -0.7677, -0.0648,  0.0781,  0.4791,  0.1179,  0.0889,\n",
       "           0.2366,  1.0789, -0.4879,  0.2880,  0.0609,  0.0473, -0.7393,  0.1511,\n",
       "           0.2593,  0.4943, -0.7499, -0.2906,  0.4928, -0.3526,  0.4419,  0.4998,\n",
       "          -0.4356, -1.7712, -0.4643,  0.7111, -0.7094,  0.2295, -1.5176, -0.5119,\n",
       "           0.5091,  0.8205,  0.9778,  0.3781, -0.6825, -0.7920, -0.4838,  0.1142,\n",
       "           0.1682,  0.1467, -0.2341,  0.1647,  1.3355, -0.3520, -0.2157, -0.7387,\n",
       "          -0.9993, -0.8559, -0.1858,  0.2644,  0.4280, -0.0036, -0.0442,  0.6877,\n",
       "          -0.8542,  0.0341,  0.6993,  0.5645,  0.3434, -0.5426,  0.5297, -0.3938,\n",
       "           0.4306, -0.3448,  0.2976,  0.0507,  0.2769, -0.7481,  0.9645,  0.7977,\n",
       "          -0.6261, -0.4071, -0.0253,  0.2362,  0.1486, -0.1955,  1.6891, -0.5366,\n",
       "          -0.2830, -0.1843,  0.1093, -0.2484, -0.1792,  0.1287,  0.3865, -0.4397,\n",
       "          -0.2211,  0.1237, -0.3107,  0.2913, -0.3128,  1.3225, -0.1474, -0.6056,\n",
       "          -0.4067,  0.0600,  0.0029,  0.4162, -0.6592,  0.2570, -1.1827, -0.2955,\n",
       "           0.2729, -0.8291,  0.2656,  0.7768, -0.0468, -0.1942, -0.1687, -0.5466,\n",
       "           0.3855, -0.3546, -0.5533,  0.1980, -0.3141, -0.0278, -0.5391, -1.0142,\n",
       "           0.0237, -0.5260,  0.3429, -0.1363,  0.2040,  0.7652, -0.3721,  0.3404,\n",
       "           0.7853,  0.6331, -0.7594,  0.5609,  0.3770, -0.1756, -0.6589, -0.3419,\n",
       "           0.1847, -0.7837, -0.1935,  0.3681, -0.7991,  0.1891, -0.8499, -0.2765,\n",
       "           0.2417,  0.0870,  0.3147,  0.0764, -0.2309, -0.2343,  0.7817, -0.0189,\n",
       "          -1.0294, -0.1408, -0.3075,  0.2752,  0.3050,  0.5806, -0.6541,  0.4985,\n",
       "          -0.1190,  0.3980,  0.5132,  0.6011, -0.0468,  0.0198, -0.4346,  0.7616,\n",
       "           0.0630, -0.4446, -0.4123, -0.1200, -0.3440, -0.6736, -0.3251,  0.0093,\n",
       "          -0.2199,  0.0942, -0.2677, -0.6290, -0.0912,  0.1891, -0.4631,  0.2098,\n",
       "           0.3130,  0.5129,  0.1729, -0.9546,  0.5442,  0.1469,  1.4524,  0.0299,\n",
       "           0.5358,  0.5689,  0.1409,  0.5944,  0.3440, -0.8182,  0.4405, -0.3603,\n",
       "          -0.0399, -0.0368, -0.0828, -0.2615, -0.7529, -0.0642,  0.5719,  1.2290,\n",
       "           0.0354, -0.3685,  0.7106,  0.8922,  0.1714, -0.1694, -1.0849, -0.0532,\n",
       "          -0.1932, -0.4340,  0.3587, -0.5552,  1.0547, -0.1309, -0.4844,  0.0147,\n",
       "           0.2591, -0.0230,  1.0043,  0.8329, -0.1950, -0.0205, -0.7525,  0.7882,\n",
       "          -0.2877, -0.1718, -0.2338,  0.0329, -0.6850,  0.5210,  0.4547,  0.6794,\n",
       "          -0.5391, -0.7085, -0.3623, -0.1240,  0.0296, -0.7097, -0.0042, -0.5575,\n",
       "           0.7436,  0.1151, -0.0573, -0.1538,  0.1445,  0.0163,  0.5612, -0.4567,\n",
       "          -0.2925,  0.6124, -0.2592,  0.5768, -0.0668,  0.4407,  0.2977,  0.0434,\n",
       "           0.0062, -0.7914,  0.2877,  0.4517, -0.0454, -0.3283,  0.7383, -0.0654,\n",
       "          -0.3579,  0.6279,  0.2042,  0.4958, -0.3704,  0.3047, -0.2352, -0.7243,\n",
       "          -1.0581,  0.7675, -0.1485,  0.2522,  0.4345, -0.0980,  0.9878, -0.5622,\n",
       "           0.2375, -0.5349,  0.2945,  0.0846, -0.0438, -0.3315,  0.3700, -0.0749,\n",
       "           0.7524, -0.7650, -0.4346,  0.2003, -1.0101, -1.0910,  0.5005,  0.2917,\n",
       "          -0.1286, -0.4802, -0.4165,  0.3200,  0.1129,  0.1481, -0.5737, -0.1174,\n",
       "           0.6047,  0.4392,  0.2555, -1.2127,  0.2207,  0.8882,  0.7224,  0.0086,\n",
       "          -0.6529,  0.3481,  0.2985, -0.6467, -0.3045,  0.1996, -0.5917,  0.2915,\n",
       "           0.5482, -0.1057, -0.1655, -0.1208,  0.2896, -0.0932,  0.3070, -0.1792,\n",
       "          -0.5291,  0.4843,  0.2117, -0.7784, -0.5277,  0.0807,  0.0301, -0.0657,\n",
       "          -0.5673, -0.1643, -0.2344, -0.3292,  0.5278,  0.5765, -0.1766, -0.1158,\n",
       "           0.1272, -0.4914, -0.6677,  0.6562, -0.1459, -0.3316, -0.5772, -0.9476,\n",
       "           0.2529,  0.2662, -0.4912, -0.4199,  0.2427,  0.5854, -0.5067,  0.7973,\n",
       "           0.1969,  0.5016,  0.3672, -0.1346,  0.1773, -0.4335, -0.1159, -0.3690,\n",
       "           0.9446, -0.1581, -0.5142,  0.0322, -0.2440, -0.3345,  0.5046,  0.0997,\n",
       "          -0.6315,  0.4327,  0.3654, -1.1732, -0.0711, -0.2270,  0.1705, -0.1113,\n",
       "           0.7538,  0.7748,  0.2552,  0.2347,  0.1843,  0.8369, -0.0429, -0.4891,\n",
       "           0.0618, -0.0674, -0.6172,  0.9644,  0.4890, -0.0317,  0.3027, -0.3260,\n",
       "           1.1614,  0.2488, -0.1971, -0.0237,  0.7769,  0.1446, -0.1081,  0.8827,\n",
       "          -0.8298,  0.2930,  0.2400,  0.6214, -0.4023, -0.1837,  0.2702, -0.0763,\n",
       "           0.0804,  0.2990,  0.0238,  0.1814, -0.4352,  0.3817, -0.2317,  0.2761,\n",
       "          -0.1215,  0.2201, -0.0225,  0.0061, -0.8005, -0.0083,  0.1447,  0.1046,\n",
       "          -0.7266,  0.3399, -0.3246,  0.0520,  0.8745, -1.5973, -0.7103, -0.9314,\n",
       "           0.7273, -0.5029, -0.2672,  0.4696,  1.2732,  0.8401, -0.4871, -0.4333,\n",
       "          -1.0375,  0.3039,  0.1068, -0.6622, -0.1389, -0.3447,  0.2598]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 3.9055e-01, -4.7271e-01,  6.6237e-01,  2.2703e-01, -4.3425e-01,\n",
       "          -2.7457e-02,  5.4540e-01,  3.8123e-01,  6.3211e-02, -9.7677e-02,\n",
       "          -1.8540e-01,  2.0380e-02, -8.0778e-01,  2.8951e-01,  2.5192e-01,\n",
       "          -1.0650e-01, -6.2661e-01,  8.6445e-01,  9.4765e-02, -6.4758e-01,\n",
       "           6.9151e-01, -3.4574e-01,  3.3921e-01,  5.4277e-01, -9.6127e-02,\n",
       "           4.0547e-02,  6.9635e-01, -4.4409e-03,  4.8675e-01,  8.6333e-02,\n",
       "          -1.1982e-01,  3.6013e-01, -3.1714e-01,  1.3672e-02,  3.3107e-01,\n",
       "           5.8126e-01,  1.9951e-01, -1.8956e-01, -7.4340e-01, -1.6114e-01,\n",
       "          -7.5059e-01, -2.6146e-01, -5.9208e-01, -2.6243e-01,  2.5324e-01,\n",
       "           6.5244e-01,  1.5418e-01,  9.1052e-01,  2.7456e-01, -7.3969e-01,\n",
       "           3.3157e-01,  2.0159e-01, -1.5626e-01, -9.1266e-01,  1.8573e-01,\n",
       "          -4.2673e-01, -6.4231e-01, -1.7539e-01, -2.8975e-01,  1.1207e-01,\n",
       "           1.4766e-01, -5.3807e-02,  1.8546e-01, -7.1132e-04,  6.7714e-01,\n",
       "          -1.3188e-01, -2.0551e-01,  9.9419e-01,  2.4772e-01,  3.2348e-01,\n",
       "          -7.1907e-04, -1.3426e-01,  1.0297e-01,  7.4892e-01, -1.2357e-02,\n",
       "           4.1795e-01,  1.7304e-01, -4.6676e-01,  1.4086e-01, -3.2317e-01,\n",
       "          -9.1183e-02, -6.2301e-01,  5.6757e-02,  4.6788e-01, -5.5287e-01,\n",
       "           4.2722e-01, -1.3719e-01,  3.7725e-01,  9.7001e-02, -5.3509e-01,\n",
       "          -1.8692e-01, -2.2820e-01,  3.3861e-01,  1.2780e-02,  1.1795e+00,\n",
       "           7.2685e-01,  2.8169e-01, -5.7840e-02, -9.9960e-02,  4.1131e-01,\n",
       "           4.0119e-02, -1.9307e-01, -2.1982e-01, -1.3021e-01, -4.3702e-01,\n",
       "           3.9378e-01, -6.7979e-02,  2.0174e-01, -6.5224e-01, -1.0181e+00,\n",
       "           1.2787e+00, -3.3047e-01,  4.7493e-01,  5.3019e-01, -4.4962e-01,\n",
       "           3.6115e-01,  2.6840e-01,  4.2514e-01,  6.7418e-03, -1.3606e-01,\n",
       "          -2.2671e-01, -3.1741e-01,  4.2021e-01, -1.8051e-01,  7.4003e-01,\n",
       "          -5.9657e-01,  1.5543e-01,  2.2894e-01, -1.6325e-01, -2.5823e-01,\n",
       "           2.6169e-01, -2.7059e-02,  4.0270e-01, -1.3332e-01,  5.0669e-01,\n",
       "          -1.0529e-01, -2.2374e-01,  7.1686e-02,  5.0587e-01,  2.6207e-01,\n",
       "           1.1230e+00,  3.5476e-01, -1.7955e-01,  6.2370e-01,  5.2474e-02,\n",
       "           5.4767e-01, -3.0046e-01,  2.3241e-01,  7.5368e-01,  3.6417e-01,\n",
       "           5.7649e-01,  1.1993e-01, -6.9451e-01,  1.4782e-02, -7.7050e-01,\n",
       "           7.7861e-01,  3.5980e-02, -2.1951e-01,  3.7678e-01, -1.7201e-01,\n",
       "           3.2115e-01,  3.3469e-01, -1.1134e+00, -1.6580e-01, -1.8953e-01,\n",
       "           3.5856e-01, -2.1123e-01,  1.3134e-01,  1.2044e+00, -7.1046e-01,\n",
       "           8.3715e-01,  2.6629e-01, -5.3742e-01, -4.3827e-01, -2.7365e-01,\n",
       "           1.0262e+00, -1.7018e-01, -1.6926e-01, -2.3264e-01,  5.3229e-02,\n",
       "           7.2815e-01, -1.7280e-01,  1.0799e-03, -1.9435e-01,  2.8994e-01,\n",
       "          -2.4731e-01, -2.0253e-01, -2.6290e-01, -4.8853e-01, -4.4694e-01,\n",
       "           1.0706e-01,  7.0610e-01,  3.8620e-01, -5.0198e-01, -8.7559e-01,\n",
       "           7.1265e-01,  9.0322e-01,  8.3671e-01,  6.1750e-01,  2.6776e-01,\n",
       "           7.4616e-01, -4.2431e-01, -2.0205e-01,  9.0658e-01, -2.8275e-01,\n",
       "           6.6997e-01, -6.2485e-02,  2.7546e-01,  1.0565e-01,  5.8918e-01,\n",
       "           5.4234e-01,  1.4987e-01,  5.5905e-01,  3.6067e-01, -2.9061e-01,\n",
       "          -7.0088e-02,  2.9937e-01, -2.6982e-01, -8.1537e-01,  4.1254e-01,\n",
       "          -3.8914e-01,  8.9954e-02,  6.4474e-01, -3.0622e-01, -3.7429e-01,\n",
       "           4.5165e-01,  1.4025e-02,  3.9800e-01, -5.6886e-01, -7.2220e-01,\n",
       "          -2.5337e-01, -3.7072e-01,  1.7300e-01, -3.0544e-01,  3.1912e-01,\n",
       "          -5.3759e-01,  1.2361e-01, -5.6342e-02,  2.0427e-01, -6.9697e-02,\n",
       "          -4.0741e-01, -7.5407e-01,  5.1874e-02, -8.3083e-01,  1.6785e-01,\n",
       "          -3.0971e-01,  6.0215e-01,  6.1853e-01, -5.0586e-01,  1.0507e-02,\n",
       "           5.2929e-01,  1.9120e-01, -2.6945e-01,  1.8169e-01,  3.7533e-02,\n",
       "           2.1499e-01,  8.7409e-01, -1.0701e-01, -3.3639e-01,  3.7929e-01,\n",
       "          -6.9926e-01,  1.8851e-01, -3.4197e-01,  1.5876e-01, -7.8849e-01,\n",
       "          -1.2318e+00,  2.4776e-01,  1.0423e-01,  3.8393e-01, -9.7971e-01,\n",
       "          -1.0883e-01, -2.8824e-01,  6.3300e-01, -2.9928e-02, -1.6727e-01,\n",
       "          -3.7538e-01,  3.2233e-01,  7.3161e-01,  1.1072e-01, -4.9650e-01,\n",
       "           6.3795e-01, -6.8291e-02, -1.8183e-02, -3.0913e-02,  1.2221e-01,\n",
       "           4.2149e-01, -1.8838e-01, -6.4440e-02, -2.6515e-01, -4.0352e-01,\n",
       "          -2.9574e-01,  3.2190e-01, -1.1050e+00,  3.9177e-01, -2.2840e-01,\n",
       "          -5.2587e-01, -3.9881e-01,  1.3074e-01, -4.9628e-01, -5.2754e-01,\n",
       "          -4.1962e-01, -1.8934e-01, -1.3741e+00,  1.4197e-01, -4.7548e-01,\n",
       "           9.7593e-02,  1.5862e-01,  1.6229e-01,  1.6065e-02, -5.0526e-01,\n",
       "           2.1731e-01, -2.5595e-01, -4.2360e-01, -1.6242e-02, -2.2740e-01,\n",
       "          -2.7088e-01,  2.1419e-02,  2.7704e-01,  3.3848e-02,  1.2690e-01,\n",
       "           5.6959e-01, -7.6501e-01, -2.9783e-01,  3.6651e-01, -5.1424e-01,\n",
       "           7.4763e-01, -1.9184e-01, -1.9584e-01, -1.3779e-04,  9.1441e-02,\n",
       "           7.7831e-01, -1.4480e-01,  7.4262e-02,  1.0029e-01,  1.2814e-01,\n",
       "          -3.3723e-01,  4.0737e-01, -3.4514e-01,  4.1096e-02, -1.9876e-01,\n",
       "           1.4986e-02, -3.7957e-01, -1.8990e-01,  4.2544e-02,  7.4278e-02,\n",
       "           3.8565e-01,  3.7686e-01, -6.4159e-02,  3.0451e-01, -3.6204e-02,\n",
       "           5.3373e-01, -8.0755e-02, -5.6977e-01,  4.0185e-01,  5.2002e-01,\n",
       "          -6.0530e-01,  1.0286e-01, -1.9804e-01,  2.2625e-01,  2.4874e-01,\n",
       "          -4.0361e-02, -3.9490e-01,  2.7177e-01, -5.9113e-03,  3.1148e-01,\n",
       "           5.7220e-01, -2.5957e-01,  2.7480e-01, -1.0242e-01, -2.7326e-01,\n",
       "           5.1181e-01, -2.5854e-01, -1.5185e-01, -3.6176e-01, -8.9687e-01,\n",
       "           9.5179e-01, -3.6545e-01, -3.5034e-01,  4.7609e-01, -4.7440e-01,\n",
       "          -4.1065e-01, -3.7107e-02, -8.9225e-01,  2.0332e-01, -1.9415e-01,\n",
       "          -1.1187e+00,  9.4547e-02,  1.6523e-03, -5.9498e-01,  1.4150e-01,\n",
       "          -4.7245e-01,  3.7548e-01,  5.6893e-01,  1.7628e-01, -5.3153e-01,\n",
       "           6.1816e-01,  7.1229e-01, -5.5144e-01, -3.5918e-02,  2.5459e-01,\n",
       "           3.4760e-01,  1.1506e+00,  1.3412e-01,  1.5511e-01,  4.5393e-02,\n",
       "          -6.8859e-02,  2.0734e-01, -8.0588e-01, -5.3778e-01,  4.4005e-01,\n",
       "          -2.2389e-01, -4.1731e-01, -1.6358e-01, -1.2821e-01,  7.6910e-02,\n",
       "          -6.5576e-02, -7.3344e-02, -3.8527e-01, -8.7211e-01, -3.3110e-01,\n",
       "          -4.4436e-01, -7.5545e-01,  1.6773e-01, -1.0234e+00,  2.8501e-01,\n",
       "          -2.1376e-01, -3.2247e-01,  1.5286e-02, -1.9768e-02, -2.4653e-01,\n",
       "           2.0748e-01, -4.6985e-01,  8.4810e-02, -1.2385e-01,  5.1138e-01,\n",
       "           6.3530e-01,  7.7122e-01, -4.6395e-01,  1.5885e-01, -7.3955e-02,\n",
       "          -4.5809e-02, -4.6950e-01,  2.4824e-01, -4.0695e-01, -4.1399e-01,\n",
       "          -2.3349e-01, -2.5024e-02,  2.6664e-01,  2.1135e-02,  2.6810e-01,\n",
       "           6.7616e-01,  7.3221e-02, -1.6618e-01,  5.5696e-02, -4.0754e-01,\n",
       "           8.5257e-03,  4.4587e-01,  1.9044e-01, -2.8082e-01,  1.0805e-01,\n",
       "          -3.8618e-01,  6.5472e-01,  3.6954e-01,  6.4515e-02,  8.9651e-02,\n",
       "          -4.1934e-01, -5.6260e-01,  2.4396e-01,  1.1315e+00, -6.3676e-01,\n",
       "           5.8259e-01,  2.6590e-02,  1.7126e-01,  2.7787e-01,  2.4315e-01,\n",
       "           7.9711e-01, -3.9691e-01,  2.6449e-01, -9.2395e-02, -3.1081e-01,\n",
       "          -1.2063e-01,  1.0641e-01,  1.9952e-01, -5.6755e-01, -8.5748e-01,\n",
       "          -1.1243e-01,  7.6769e-01, -5.1312e-01,  4.9419e-01,  9.9051e-02,\n",
       "           8.2486e-01, -5.3776e-01,  1.1387e-02,  6.4653e-02, -4.2799e-01,\n",
       "          -7.7336e-01,  3.9238e-01,  6.6260e-01,  1.3111e-01, -2.1117e-01,\n",
       "          -3.8192e-01, -2.5726e-01,  3.3019e-03, -7.6804e-01, -5.7466e-01,\n",
       "           2.0508e-01, -1.0387e-01,  8.2538e-01,  1.7548e-01, -4.1273e-01,\n",
       "           7.0200e-01,  6.5339e-01, -6.6564e-01,  3.6919e-01,  9.4752e-01,\n",
       "          -5.6704e-01, -2.6721e-01, -5.4506e-01,  3.6876e-01,  6.0872e-01,\n",
       "          -1.6375e-01,  2.7380e-02,  5.2723e-01, -5.0001e-01,  4.3866e-01,\n",
       "           2.2265e-01,  5.3524e-01,  3.5670e-01,  4.1932e-01,  2.6395e-01,\n",
       "           6.9779e-02,  6.0878e-01,  2.0751e-01, -6.1003e-01, -5.3953e-01,\n",
       "           1.6769e-02,  1.8370e-01, -1.3123e-02,  7.0484e-01,  1.1441e+00,\n",
       "           4.7635e-01, -8.1326e-03,  6.0900e-01, -2.7674e-01,  3.5816e-01,\n",
       "          -5.0320e-01,  7.5124e-01,  3.3806e-01, -7.2863e-02,  2.8416e-01,\n",
       "          -2.1141e-01, -1.1820e+00, -8.7308e-01,  2.4283e-01,  2.2788e-01,\n",
       "          -4.2546e-01,  6.1860e-01,  6.9749e-01, -8.1833e-02, -1.1800e-01,\n",
       "           7.5882e-01,  1.4722e+00,  3.0993e-01, -6.0146e-01, -3.9154e-01,\n",
       "          -5.5242e-01,  6.9138e-02, -6.2611e-01,  3.3228e-02, -2.8773e-01,\n",
       "          -1.2910e+00,  2.5446e-01, -9.9296e-02,  6.2136e-01, -3.4945e-01,\n",
       "          -1.6617e-02, -7.9631e-01, -7.1384e-01, -5.0761e-01, -3.5295e-02,\n",
       "           2.3656e-01, -1.4415e+00, -3.0933e-01, -3.3281e-02, -3.6114e-01,\n",
       "          -2.4080e-01, -2.5934e-01, -4.8486e-01, -1.9639e-01, -3.8505e-01,\n",
       "           6.5496e-01,  2.4683e-01,  4.3819e-01, -4.3059e-01,  4.2479e-01,\n",
       "           8.9591e-02,  3.5214e-01,  4.1598e-01,  7.8812e-01, -6.0098e-01,\n",
       "          -6.8519e-01, -1.6867e-01, -3.8392e-01, -3.6650e-01, -1.7263e-01,\n",
       "          -6.4631e-01, -2.7939e-01, -1.7345e-01, -1.1135e-01, -4.1478e-01,\n",
       "           1.0718e-01, -9.4019e-02,  3.9969e-01,  5.4917e-01, -1.2500e-01,\n",
       "          -7.9812e-01,  6.3853e-01,  1.6948e-01,  2.6011e-01, -1.7932e-01,\n",
       "           3.7860e-01, -1.0075e-01, -8.9571e-02, -1.5634e-02, -4.1872e-01,\n",
       "           1.0797e-01, -5.6367e-02, -4.7924e-01,  4.6807e-01,  3.3286e-01,\n",
       "          -2.8743e-01, -1.5687e+00, -1.7528e-01,  2.7327e-01,  3.8133e-01,\n",
       "           4.0918e-01, -4.4587e-02,  2.6540e-01,  7.6914e-02, -3.9409e-01,\n",
       "           1.0341e+00, -8.4502e-01, -2.8871e-02,  7.9762e-01, -9.1942e-01,\n",
       "          -1.4864e-01, -9.2066e-01,  4.1165e-01,  3.0677e-01, -5.1563e-01,\n",
       "           2.4374e-02, -1.9077e-01,  5.4063e-01, -3.1902e-01, -1.6691e-01,\n",
       "          -2.9160e-01,  4.8521e-01, -4.0744e-01, -2.8932e-01, -2.3034e-01,\n",
       "           4.5859e-01, -4.2843e-01, -7.3078e-02,  3.8518e-01, -5.3692e-01,\n",
       "           1.2437e-02,  2.6880e-01, -1.8749e-02,  2.6603e-01, -4.2977e-02,\n",
       "          -3.0773e-01, -1.1798e+00,  3.9151e-01,  6.5816e-01,  2.7282e-01,\n",
       "          -8.3982e-01,  3.6669e-01, -3.3612e-01, -7.5315e-01,  2.7461e-01,\n",
       "          -1.5669e-01,  4.4385e-01,  3.9927e-01,  2.1873e-02,  4.1783e-01,\n",
       "          -8.0600e-02,  5.3185e-01,  3.5647e-01, -1.8551e-01,  4.7164e-01,\n",
       "          -2.7067e-01, -5.4123e-01,  6.5461e-01,  1.0995e-01, -5.4790e-01,\n",
       "           2.2664e-01, -9.3967e-01,  3.1207e-01,  1.1711e-01, -2.0889e-01,\n",
       "           5.1832e-03, -4.4923e-01, -9.5710e-01,  3.9283e-02,  1.6976e-01,\n",
       "          -3.5182e-01,  2.9762e-01, -2.5745e-01,  6.9950e-01, -8.3363e-01,\n",
       "           1.0860e-01, -5.0131e-02, -2.3116e-01, -3.8105e-02, -4.5216e-01,\n",
       "           4.7045e-01, -1.1034e-01,  5.4412e-01, -3.4912e-01, -1.1833e-01,\n",
       "          -3.6111e-01, -1.9894e-01, -6.3623e-01, -3.7916e-01,  2.8288e-01,\n",
       "          -5.9326e-01, -3.2103e-01,  1.3909e-01,  3.5165e-01, -4.3108e-01,\n",
       "          -3.6297e-01,  9.8478e-01,  2.0609e-01, -2.0909e-01, -6.9887e-01,\n",
       "          -8.1687e-02,  6.5679e-01, -2.8129e-01,  7.7820e-02, -4.5033e-02,\n",
       "           3.4612e-01,  1.0955e-01,  2.3420e-01, -7.1898e-01, -9.2786e-01,\n",
       "          -3.6412e-02,  1.0002e-01, -2.6996e-02,  4.9630e-01,  4.3167e-01,\n",
       "           3.0318e-01]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 5.3830e-01, -1.0477e-01,  1.6453e-01, -6.4605e-01, -4.9556e-01,\n",
       "          -2.5655e-02, -6.4864e-02, -3.6879e-01,  2.4452e-01, -4.9547e-01,\n",
       "           1.6673e-01, -3.8494e-01, -2.5706e-01, -9.6352e-02,  1.8000e-01,\n",
       "           1.3339e-01, -2.2529e-01, -2.8951e-01,  5.9108e-02, -1.4545e-01,\n",
       "          -1.4339e-01,  4.8050e-01,  4.5216e-01, -3.0895e-01, -1.3235e-01,\n",
       "          -1.6676e-01,  3.5421e-01, -1.3746e-01,  1.3012e-02, -7.8216e-01,\n",
       "           4.3977e-02,  3.1222e-01,  1.8040e-01,  5.5159e-01,  3.8823e-01,\n",
       "          -1.9096e-01,  2.9002e-01, -7.5944e-01, -1.2317e+00, -6.6131e-01,\n",
       "          -2.1304e-01,  1.1064e-01,  2.3698e-02,  2.8187e-01, -6.0954e-03,\n",
       "           2.7299e-01,  2.1296e-01,  3.7303e-01, -2.1298e-01,  1.8716e-01,\n",
       "          -3.0036e-01, -7.9677e-01, -3.2252e-01, -2.0402e-01,  5.4663e-02,\n",
       "          -8.6676e-02,  7.1887e-01,  6.1851e-01, -2.0506e-01, -1.8818e-01,\n",
       "          -4.4177e-01, -6.3503e-03,  2.0070e-02,  1.0522e+00, -1.2949e-01,\n",
       "           2.9085e-01, -2.1320e-01, -7.6706e-02, -1.9900e-02, -4.0629e-01,\n",
       "           3.7525e-02,  5.7746e-01, -2.6580e-01, -8.4052e-02, -1.8910e-01,\n",
       "           1.5960e-01,  1.3132e-01, -6.2838e-01,  1.0013e-01, -4.3669e-01,\n",
       "          -4.7294e-01,  3.7902e-01, -2.9011e-01,  2.2315e-01, -7.8256e-01,\n",
       "           2.8129e-02,  3.0717e-01,  6.2194e-02, -1.8933e-01, -4.0150e-01,\n",
       "          -3.7876e-01, -1.5449e-01,  6.3511e-01,  2.8520e-02,  2.8359e-01,\n",
       "           9.6656e-02, -5.7774e-01, -3.3435e-01, -4.0209e-01, -2.9895e-01,\n",
       "           1.4235e-01, -3.5013e-01,  6.8936e-01,  6.5540e-01,  8.4975e-02,\n",
       "          -5.8508e-01, -3.4284e-01,  3.1670e-01,  9.6649e-02, -3.3757e-01,\n",
       "           1.5992e-01,  2.0418e-01,  2.3542e-01,  5.2642e-01, -3.9584e-02,\n",
       "           1.2910e-01,  3.0309e-01,  1.1152e+00,  3.4423e-02, -2.9781e-01,\n",
       "           3.1837e-02,  5.8259e-01,  1.9631e-01,  2.3263e-01,  1.7203e-01,\n",
       "          -6.4716e-01,  3.1381e-01,  6.3948e-02, -5.2955e-01, -7.3714e-01,\n",
       "           2.4751e-01,  2.9379e-01, -4.2638e-01, -6.3334e-02,  1.8751e-01,\n",
       "           6.8993e-02, -6.4242e-01, -3.3756e-01, -4.3000e-02, -1.1051e-01,\n",
       "           1.0577e-02,  1.9277e-01,  2.9660e-01, -3.3903e-01,  2.4220e-01,\n",
       "           3.9596e-01, -1.2120e-01,  5.2425e-01, -4.6007e-01,  8.6630e-03,\n",
       "           2.8549e-01,  5.8032e-02,  4.0697e-02, -3.4815e-01, -1.7209e-01,\n",
       "          -2.5043e-01, -6.2715e-01,  9.7840e-02, -3.3055e-02, -2.5347e-01,\n",
       "           1.0887e+00,  8.1536e-02,  3.9916e-02,  1.1472e+00,  4.9545e-01,\n",
       "           1.3008e-02,  8.6340e-02,  2.2148e-01, -3.0711e-01, -2.9355e-01,\n",
       "          -2.7086e-01, -4.3947e-01, -3.7345e-01,  6.2023e-01, -2.5486e-01,\n",
       "           4.6629e-01, -7.1173e-01, -1.7978e-01, -7.4610e-02, -2.1812e-01,\n",
       "           1.7157e-01,  7.8158e-01, -3.4750e-01,  3.6170e-01, -9.4527e-02,\n",
       "          -1.6140e-01, -3.5356e-01,  1.0262e-02,  4.5317e-01, -5.5217e-01,\n",
       "           3.8764e-01, -2.5565e-01, -2.7423e-01,  4.2255e-01,  1.5319e-01,\n",
       "          -2.2859e-01, -4.7438e-02, -8.3057e-03,  2.6511e-01,  4.7559e-02,\n",
       "          -2.3570e-01, -1.5263e-01,  7.6781e-01, -4.0134e-01,  5.2183e-01,\n",
       "          -2.5687e-01, -3.6293e-01, -7.3186e-01,  5.6177e-01,  2.0615e-01,\n",
       "           2.8855e-01, -7.9567e-02,  1.1627e-01,  5.6212e-01, -3.0831e-01,\n",
       "          -4.6127e-02, -4.5934e-01,  1.8822e-01, -5.7643e-01, -4.1924e-01,\n",
       "           2.0991e-01,  1.3020e-02, -1.9701e-01,  6.6311e-01, -9.2408e-01,\n",
       "          -6.5607e-01,  2.5312e-01, -4.9313e-01,  1.5174e-01,  6.5698e-01,\n",
       "          -1.0918e-01, -1.3105e-01,  4.7297e-01,  6.4013e-02,  2.2186e-01,\n",
       "          -4.6388e-01,  2.3823e-01, -7.8960e-02, -3.6204e-01,  7.1561e-01,\n",
       "           2.2489e-01,  3.9475e-02, -6.5066e-02, -1.5671e-01, -1.8958e-01,\n",
       "           4.8386e-01,  3.2840e-01, -1.3833e-01, -2.1327e-01, -3.8553e-01,\n",
       "           7.1837e-01, -4.0283e-01, -3.4070e-01,  1.1794e-01, -2.7124e-01,\n",
       "           3.9021e-01,  3.8227e-01, -2.1989e-01,  9.1296e-01,  6.7768e-02,\n",
       "          -1.6942e-01, -8.6603e-01, -6.5631e-01,  1.2301e-01, -1.4986e-01,\n",
       "           3.8464e-01,  2.6970e-01, -1.1751e-01, -6.6083e-01, -5.1163e-01,\n",
       "           7.4098e-01, -5.1281e-01, -2.2815e-01, -1.7049e-01,  2.0226e-01,\n",
       "           5.9915e-01, -3.8714e-01,  5.0124e-01,  6.2820e-01, -6.8876e-01,\n",
       "          -4.7108e-01, -3.9995e-04, -1.8745e-01, -1.1021e+00, -2.0074e-01,\n",
       "           5.4547e-03, -3.0583e-01,  5.1668e-01,  3.0838e-01, -4.7118e-02,\n",
       "          -4.4413e-01,  2.4180e-01,  3.3150e-01,  2.1495e-01, -2.4999e-01,\n",
       "           1.0390e-01, -2.8399e-01,  9.3149e-02,  8.2283e-01, -2.6164e-02,\n",
       "           2.4359e-01,  1.9914e-01, -1.3424e-01, -3.0736e-02,  2.6212e-01,\n",
       "          -4.9211e-01, -2.1837e-01,  1.3530e-03, -5.4055e-01, -3.6086e-02,\n",
       "           1.8558e-01,  3.1775e-01, -3.1255e-01, -3.5370e-01,  2.9534e-03,\n",
       "           5.9540e-02,  1.4980e-01, -5.0851e-01,  3.4766e-01,  4.5907e-01,\n",
       "          -8.6498e-02, -6.6555e-02, -3.5569e-01, -6.2760e-01,  7.7365e-01,\n",
       "           4.5534e-02, -1.8480e-01, -7.7071e-01,  2.2391e-01,  1.2770e-01,\n",
       "           1.7506e-01,  2.9258e-01,  1.1827e-01,  2.9830e-01,  3.1763e-01,\n",
       "           3.0195e-01, -9.5265e-02, -1.7716e-01,  1.5814e-01, -6.5565e-01,\n",
       "           3.5504e-01,  4.0142e-01, -6.7534e-02, -7.1110e-01,  5.2255e-02,\n",
       "          -3.8348e-01, -6.1552e-02,  5.2445e-01, -1.2185e-01,  4.0810e-01,\n",
       "           5.1100e-01,  7.6169e-01, -3.5914e-01,  4.6147e-02, -1.0348e+00,\n",
       "           3.0366e-01, -3.1153e-01,  5.2838e-02, -3.8897e-03, -7.2015e-03,\n",
       "          -3.1149e-01,  5.7547e-01, -1.1629e-01, -1.5529e-02, -1.7218e-01,\n",
       "           1.3128e-01, -7.9029e-01,  1.6577e-01, -4.4125e-01,  5.5189e-01,\n",
       "          -4.9599e-02, -1.2442e-01,  6.3593e-02, -3.0720e-01,  6.9716e-01,\n",
       "          -1.1472e+00,  1.9430e-01, -8.2645e-01,  2.5627e-01, -2.6934e-01,\n",
       "           5.1903e-01, -1.8435e-02,  3.1081e-01, -3.5406e-01,  2.0370e-02,\n",
       "           3.3755e-01,  6.7597e-01,  1.5389e-01, -7.1122e-01,  7.9318e-02,\n",
       "           4.4304e-01, -2.8049e-01,  4.4483e-01, -4.7645e-02, -3.0190e-02,\n",
       "           3.6489e-01, -1.4100e-01, -1.7700e-01,  1.2517e-01, -3.8310e-01,\n",
       "           5.5479e-01,  1.8210e-01,  1.4213e-02, -8.0945e-02, -2.1778e-02,\n",
       "           2.9301e-01, -6.1460e-01,  2.4697e-01, -2.0737e-01,  3.9029e-01,\n",
       "           3.5893e-01, -2.2011e-02, -3.7052e-01,  4.6465e-01,  3.1202e-01,\n",
       "          -7.0808e-01,  8.9654e-01, -1.5368e-01, -2.2135e-01,  2.4999e-01,\n",
       "           2.4641e-01,  3.8026e-01,  1.8340e-01,  6.3957e-01, -1.2811e-01,\n",
       "           7.5523e-02,  1.6461e+00,  4.6373e-01,  3.5971e-01, -4.8999e-01,\n",
       "           4.0406e-01, -6.8274e-01,  6.9106e-02,  1.4667e-01,  1.8135e-01,\n",
       "          -4.7380e-04,  6.2379e-02,  5.6946e-01,  5.8248e-01, -6.8443e-01,\n",
       "           1.8743e-01,  6.5623e-02, -6.4629e-01, -5.5194e-01, -2.3376e-02,\n",
       "           2.4079e-01, -4.9109e-01, -2.2920e-02, -2.5441e-02, -3.8741e-01,\n",
       "          -2.1923e-01, -5.6229e-01, -9.7378e-02,  5.7469e-02, -2.3387e-01,\n",
       "           2.8154e-01, -1.9435e-01,  5.5766e-01, -6.6048e-01,  3.6038e-01,\n",
       "          -4.0580e-01,  4.3482e-01,  4.1072e-01,  1.6970e-01, -2.6145e-01,\n",
       "           2.6771e-01, -2.3352e-01,  7.2736e-01,  5.7050e-01,  4.4033e-01,\n",
       "          -1.8358e-01,  2.8235e-01,  2.3031e-01, -3.1211e-02,  9.6157e-02,\n",
       "           5.9740e-02, -5.0921e-01, -4.0967e-01,  8.3906e-01, -4.0762e-01,\n",
       "           4.3034e-01,  3.0777e-01,  1.2867e-03,  4.3101e-01,  8.1542e-01,\n",
       "           3.2712e-02,  4.5030e-01, -2.2717e-02,  5.8616e-01, -1.0283e-01,\n",
       "           5.1702e-01,  2.2379e-01, -5.0725e-02, -7.2685e-01,  1.1416e-01,\n",
       "          -8.2116e-02,  1.5040e-01,  3.3134e-01, -7.3461e-01,  4.1241e-01,\n",
       "           4.7221e-01, -7.1686e-01,  4.7462e-01, -5.0633e-01,  9.5058e-02,\n",
       "           7.3721e-02,  1.6990e-01,  9.0091e-01, -1.5665e-01,  8.5951e-02,\n",
       "          -3.7962e-02,  7.7502e-02,  4.6733e-01,  5.7775e-01,  7.8061e-02,\n",
       "          -2.9230e-01, -8.5842e-02, -3.0426e-01,  1.8323e-01, -9.5107e-01,\n",
       "           2.4655e-01,  6.4386e-01, -2.7776e-01, -1.9212e-01,  1.3525e-01,\n",
       "          -4.5446e-01, -4.9587e-01,  1.6706e-01,  7.9093e-02, -3.4729e-02,\n",
       "          -2.6072e-01, -1.7604e-01, -3.7485e-02,  4.6345e-01, -7.9120e-01,\n",
       "           3.2304e-01,  6.0878e-02, -1.2384e-01, -5.1659e-01,  4.2616e-02,\n",
       "           4.3646e-01,  3.8319e-01, -3.1723e-01,  3.5213e-01, -3.7983e-01,\n",
       "           7.5741e-01,  5.1326e-01, -3.5340e-01,  3.5846e-01,  6.5519e-01,\n",
       "          -3.9840e-01, -5.3466e-01, -1.2979e-01, -3.0175e-02,  9.7183e-02,\n",
       "          -1.1717e-01, -4.3025e-01,  5.7875e-01,  7.3058e-04, -5.3454e-01,\n",
       "           2.9799e-01,  8.2366e-01, -2.1616e-01,  5.2221e-01,  4.8804e-01,\n",
       "           7.5288e-01, -1.0091e-01, -6.5933e-01,  1.4619e-01, -4.8216e-02,\n",
       "          -1.6981e-01, -1.2465e-01,  3.7229e-02,  1.1800e-01,  1.2578e-02,\n",
       "           1.9778e-01, -3.5879e-01, -4.1333e-01, -2.7076e-01, -1.4892e-01,\n",
       "           2.9831e-01,  7.6634e-01,  1.3108e-01, -2.9125e-01, -4.4945e-01,\n",
       "           1.0961e-01,  4.1672e-01, -3.3725e-01,  3.9845e-01,  5.9978e-01,\n",
       "          -4.6516e-01, -1.5900e-01,  3.6389e-01,  1.1514e+00,  7.0528e-01,\n",
       "          -2.0518e-01,  4.4317e-01,  3.8809e-01,  4.0652e-01,  3.6192e-01,\n",
       "          -4.4075e-01, -1.7352e-01,  4.3231e-02, -2.4090e-01, -1.0624e-01,\n",
       "          -3.9921e-01,  5.6125e-01,  8.6464e-01,  1.4818e-01,  2.2392e-01,\n",
       "          -4.6732e-01,  5.4156e-01,  3.1302e-01, -5.3453e-01,  3.8243e-01,\n",
       "          -1.2176e-02, -3.5887e-01,  3.7310e-01, -3.8286e-01, -3.7226e-02,\n",
       "          -2.1176e-02, -9.4837e-02,  4.7028e-01, -1.6699e-01,  2.1473e-01,\n",
       "           2.3508e-01, -8.6557e-02, -1.4467e-01,  1.9358e-01,  1.0081e-01,\n",
       "           7.4625e-02, -2.1634e-01,  2.2381e-01, -1.8362e-01,  9.0610e-02,\n",
       "           3.9926e-01,  8.2614e-02,  7.3579e-02,  5.9176e-01, -3.7792e-01,\n",
       "           1.7271e-01, -7.7626e-02, -6.5388e-01,  5.8031e-01, -2.8779e-01,\n",
       "           1.2171e-01,  6.6565e-02, -2.8979e-02,  4.7524e-01, -2.5754e-01,\n",
       "           6.8033e-01,  7.9705e-01, -3.6181e-01, -1.2895e-01,  2.3432e-01,\n",
       "           2.6626e-01,  1.1974e-03,  1.2490e-01,  6.2713e-02,  1.7859e-02,\n",
       "           4.9700e-01, -5.0154e-01,  4.6973e-01,  4.4740e-01,  1.0383e-01,\n",
       "           1.0055e+00, -2.6497e-01, -1.2978e-01, -3.8449e-01,  3.2897e-01,\n",
       "          -4.7552e-01, -2.1798e-01,  4.4149e-02, -4.3685e-01,  1.5278e-01,\n",
       "          -1.7082e-01, -4.9881e-01, -1.2347e-01, -4.5913e-01, -3.0301e-01,\n",
       "           3.1265e-01,  1.5895e-01,  5.6627e-01,  4.3606e-01,  3.4720e-01,\n",
       "          -6.1652e-02,  6.8027e-02, -1.5123e-01,  4.6923e-02, -6.9993e-01,\n",
       "          -2.4735e-01, -1.7302e-02, -3.4443e-01, -6.3049e-02, -6.9851e-02,\n",
       "          -8.3717e-02,  9.7816e-02, -4.6262e-01,  1.3691e-01, -1.0519e-01,\n",
       "           2.9873e-01, -7.9145e-01, -5.8368e-01,  3.7176e-01,  3.6127e-01,\n",
       "           2.3330e-01,  3.2301e-01, -1.9521e-01, -4.3637e-01,  4.1433e-02,\n",
       "           2.9561e-01,  3.9372e-01, -3.6938e-02,  1.1651e-01,  6.5484e-01,\n",
       "           1.1024e+00, -6.9093e-01,  2.3707e-02, -3.7811e-01,  1.6118e-02,\n",
       "          -1.2353e-01, -9.4680e-02,  2.0067e-01, -4.6089e-01,  3.8505e-01,\n",
       "          -3.7602e-01, -3.4141e-01, -5.0682e-01, -1.3190e-01, -3.9162e-01,\n",
       "           6.6728e-01,  1.6762e-01, -2.5295e-02, -1.6359e-01, -4.5000e-01,\n",
       "           5.3881e-01,  7.2280e-01, -1.4576e-01,  1.7714e-01, -4.0222e-02,\n",
       "           6.4721e-01,  4.0391e-01, -4.3702e-01,  2.7792e-01,  1.2833e-01,\n",
       "          -8.8939e-01,  2.3561e-01,  1.2203e-01, -3.7549e-01,  5.8894e-02,\n",
       "          -1.6130e-01]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 6.6098e-03, -2.6269e-01, -3.6757e-01,  1.7428e-01, -2.6518e-01,\n",
       "           4.3314e-01,  2.4697e-01,  2.6947e-01, -5.9036e-02,  5.7190e-01,\n",
       "          -5.9832e-02, -4.8796e-02, -2.8979e-01, -3.9205e-01, -9.1533e-02,\n",
       "           2.4398e-01, -5.3589e-01, -5.2067e-01, -1.3030e-01, -3.5642e-01,\n",
       "           3.7826e-01,  1.4606e-01, -4.9172e-01,  2.3304e-01, -6.1462e-01,\n",
       "          -1.7257e-01,  1.3017e-01,  2.5859e-01, -5.1319e-01, -9.6560e-01,\n",
       "          -2.1537e-01, -1.0612e-01,  5.6455e-01, -4.3853e-01,  1.4255e-01,\n",
       "          -4.0678e-01,  4.7528e-02, -1.6703e-01,  5.3795e-01,  4.4742e-02,\n",
       "          -2.5389e-01,  5.4649e-01,  6.6507e-01, -2.6701e-01, -9.6043e-01,\n",
       "           7.8897e-01,  2.4406e-01, -8.4904e-02,  5.3577e-01, -8.7962e-02,\n",
       "          -3.0677e-02,  4.0729e-01,  1.2955e-01, -7.3304e-02,  3.5751e-01,\n",
       "           5.6074e-01,  3.5199e-01,  2.5964e-01,  4.2081e-01, -8.4757e-01,\n",
       "          -8.7306e-01,  5.7737e-01, -4.6532e-01, -7.9193e-01,  4.2381e-01,\n",
       "          -4.0013e-01,  2.6053e-01,  9.3079e-02, -2.2615e-01,  1.5761e-02,\n",
       "           7.7065e-02, -3.9784e-01, -4.4301e-01,  2.9364e-01, -4.9937e-01,\n",
       "          -7.1365e-04,  2.6786e-01,  4.8740e-01, -6.2689e-01, -4.0611e-01,\n",
       "           2.5773e-02, -1.2259e+00,  6.7416e-01, -1.1482e-01,  3.6000e-01,\n",
       "           1.0722e+00,  1.1417e-01, -2.8620e-01, -3.4776e-02, -2.6438e-01,\n",
       "           1.5635e-01, -9.9122e-01,  3.1708e-01, -2.3045e-01, -1.7947e-01,\n",
       "          -1.7583e-01,  7.3137e-01, -2.7965e-01, -6.2029e-02, -4.9744e-01,\n",
       "          -7.2074e-03,  5.7763e-01, -2.2316e-02,  3.4636e-01, -6.2399e-02,\n",
       "          -4.6152e-01, -3.3093e-01,  3.2920e-01, -4.8017e-02, -9.5019e-02,\n",
       "           6.7077e-01, -2.0673e-01, -6.8746e-01, -8.2165e-01,  6.3699e-01,\n",
       "           5.2688e-01,  1.7991e-01, -3.1029e-01,  1.0151e-01, -3.9171e-01,\n",
       "           3.7051e-01, -3.0271e-01,  9.3095e-03, -2.8635e-01,  5.3006e-01,\n",
       "           1.9090e-01, -7.9843e-02,  4.0651e-01, -3.0271e-01,  1.8457e-01,\n",
       "           8.1496e-01, -2.9552e-01, -9.2587e-01, -1.0831e+00,  4.3485e-01,\n",
       "          -1.3011e-01, -5.7112e-01, -1.6488e-02, -2.5106e-01,  1.0652e-01,\n",
       "          -4.8980e-01, -1.2497e-01,  5.2123e-02, -1.4011e-01, -5.2127e-01,\n",
       "          -4.0007e-01, -4.2865e-01,  1.5225e-01,  3.1355e-02, -1.5135e-01,\n",
       "           3.8813e-01,  4.2117e-01,  3.5740e-01, -6.6490e-02,  2.1220e-01,\n",
       "           5.1068e-01,  3.2562e-01,  4.5427e-01,  5.3457e-01,  7.9090e-02,\n",
       "          -3.8775e-01, -5.2153e-01,  6.7552e-01, -2.4292e-01, -2.4014e-01,\n",
       "           2.3907e-01, -3.4674e-02, -4.4595e-01,  3.6620e-01,  9.5181e-01,\n",
       "          -5.1952e-02, -1.0833e-01, -5.9030e-02,  2.4807e-01, -1.1512e-01,\n",
       "           8.9181e-01,  7.3330e-03,  4.4445e-01, -3.5469e-01, -1.2251e-01,\n",
       "           6.4225e-01,  1.0712e-02,  4.2754e-01, -1.9610e-01,  4.1158e-01,\n",
       "          -6.4251e-01, -1.7617e-01, -1.3139e+00,  1.5998e-01,  4.0109e-01,\n",
       "           1.2470e-01,  3.8818e-01,  1.3401e-01,  2.7277e-01, -8.1460e-01,\n",
       "           6.1011e-01,  1.7890e-01,  4.0465e-01,  2.6107e-01,  5.3331e-01,\n",
       "          -5.9175e-01,  2.0733e-01, -3.8351e-02, -1.4117e-01,  6.4637e-01,\n",
       "          -4.1270e-01,  1.5513e-01,  7.8101e-01,  2.9674e-01, -3.9017e-01,\n",
       "          -6.0594e-01,  2.1311e-01, -7.1650e-02, -3.1542e-01,  1.7556e-02,\n",
       "          -3.5006e-01,  6.3362e-01,  1.3739e-01,  9.6715e-01, -3.4545e-02,\n",
       "          -2.1292e-01, -5.6229e-02,  8.4410e-01,  3.6312e-01,  5.4532e-01,\n",
       "          -5.8060e-01, -8.5618e-01,  4.6660e-01,  7.9354e-01, -3.5037e-01,\n",
       "          -1.3094e+00, -3.2083e-01, -9.3518e-02, -3.1536e-01, -3.2247e-01,\n",
       "          -6.4966e-01, -4.6867e-01,  1.6924e-01,  8.2207e-03,  1.9041e-01,\n",
       "           5.1591e-02,  4.5381e-01,  3.2598e-01,  2.1610e-01,  6.1546e-01,\n",
       "          -3.0557e-01,  2.3080e-01, -8.1150e-01, -1.9630e-01, -2.5474e-01,\n",
       "          -5.3179e-02,  2.0568e-01,  3.6297e-01, -3.0519e-01, -3.9749e-01,\n",
       "           3.7625e-01,  1.7798e-01, -3.1578e-01,  2.2242e-01,  6.6174e-02,\n",
       "           6.9049e-02,  5.0955e-01,  2.1540e-01, -4.4644e-01,  1.4161e-01,\n",
       "          -1.3063e-01, -2.0845e-01, -4.6655e-01,  2.1990e-01, -1.2871e+00,\n",
       "           8.3848e-02, -1.7965e-01, -2.6664e-01, -5.8816e-02,  3.4568e-01,\n",
       "          -6.5598e-01,  4.3552e-01, -5.4896e-01,  5.8596e-01,  3.8323e-01,\n",
       "           9.2886e-02,  1.0526e-01,  1.1105e-01, -1.1038e-01, -1.6629e-01,\n",
       "           6.9856e-01, -2.3793e-01,  1.1438e-02, -1.0019e-01, -2.1541e-01,\n",
       "          -2.9451e-01,  1.3113e-01, -6.5257e-02,  4.9503e-02, -5.8687e-02,\n",
       "           8.3018e-01,  7.7874e-01,  2.9874e-02,  1.5291e-01, -1.5574e-02,\n",
       "           8.5174e-01,  1.3821e-01,  2.9102e-01, -2.7723e-01,  3.0093e-01,\n",
       "          -2.2877e-01, -1.8850e-01, -7.4111e-02,  1.1164e+00,  1.1988e+00,\n",
       "           4.4152e-01, -2.1490e-01,  6.0457e-01, -4.2109e-01,  2.3614e-01,\n",
       "           1.1429e-01, -4.9072e-01, -1.9878e-01,  2.7834e-02, -3.8201e-01,\n",
       "           1.2811e-01,  1.8473e-01, -2.8794e-01,  1.2330e-01,  6.7726e-01,\n",
       "          -8.4565e-02,  5.6560e-01,  2.9893e-01,  5.1259e-01, -2.0105e-01,\n",
       "          -2.5310e-01,  1.5046e-01, -3.5782e-01, -1.3406e-01,  1.7485e-01,\n",
       "          -7.3333e-01,  2.5906e-01,  6.9018e-01, -3.7954e-01, -3.0585e-02,\n",
       "           1.0650e-02,  1.0432e-01, -6.2071e-01,  3.1720e-01, -7.5162e-01,\n",
       "          -2.9859e-01,  7.0443e-01,  5.5641e-01,  1.7026e-01, -7.5429e-01,\n",
       "          -3.9957e-01,  6.3428e-01,  6.0106e-01, -3.4590e-01,  5.0869e-01,\n",
       "           6.4531e-01,  3.3842e-01,  6.0615e-02, -5.1140e-01,  1.4446e-01,\n",
       "           4.1334e-01,  4.4533e-01,  1.0317e-01,  2.8089e-01, -5.9215e-01,\n",
       "          -2.4128e-01, -5.9714e-01,  1.2207e-01,  1.7208e-01,  7.7701e-01,\n",
       "          -1.6226e-02,  9.6592e-01, -5.7569e-01,  1.5259e-01, -5.1809e-01,\n",
       "          -1.5889e-02, -5.2705e-01,  4.6404e-01, -1.9509e-02, -4.1084e-01,\n",
       "           3.5605e-01,  3.7763e-01,  4.2387e-02, -8.1834e-01,  4.6297e-01,\n",
       "           9.3624e-02, -1.2096e-01,  1.3179e-01, -1.2753e-01, -2.3419e-01,\n",
       "          -1.9714e-01, -5.6879e-01, -1.2140e-01, -1.3627e-02, -4.6581e-01,\n",
       "          -3.4865e-01, -4.7122e-01,  9.3592e-01, -7.8973e-01, -6.1497e-01,\n",
       "           4.8625e-01, -3.7860e-01, -1.5961e-01,  4.4653e-02,  1.9781e-01,\n",
       "          -7.6400e-02, -5.5152e-01, -3.4865e-01, -1.7916e-01, -1.5426e-02,\n",
       "           8.3038e-02,  3.1429e-01, -1.6774e-01,  6.9454e-01, -1.4650e-01,\n",
       "           3.9149e-02, -3.8862e-02,  3.0923e-01, -4.9790e-02, -1.0587e+00,\n",
       "          -2.3074e-01, -4.6802e-01, -4.6939e-01,  1.3915e-01, -1.1327e-01,\n",
       "          -3.7828e-01,  7.5762e-01, -3.3760e-01,  2.4776e-01, -8.6143e-01,\n",
       "           1.8338e-01, -3.4882e-01,  2.3242e-01,  8.1804e-02,  1.0254e-01,\n",
       "          -3.2838e-01, -4.7021e-01, -1.9430e-01, -3.9120e-01, -2.7814e-01,\n",
       "          -7.1617e-01,  1.1852e+00, -3.7587e-01,  2.7408e-01, -1.2657e-01,\n",
       "          -4.2230e-01,  2.4140e-02, -6.3804e-01, -2.5995e-01,  7.3536e-01,\n",
       "           6.0698e-01,  7.9821e-02, -1.3644e-01, -8.1253e-01, -2.3465e-01,\n",
       "          -6.0299e-02, -7.9998e-01,  1.0665e+00, -2.2966e-01,  3.6236e-02,\n",
       "          -6.0286e-01, -3.4030e-01, -8.3468e-01,  4.8108e-01,  1.6565e-01,\n",
       "           6.6034e-02, -8.5642e-02,  2.0889e-01,  4.8720e-01,  1.3584e-01,\n",
       "          -3.6575e-01, -7.6117e-01, -6.6665e-01,  1.3797e-01, -5.4285e-01,\n",
       "           1.1780e-01,  3.7120e-01, -7.9084e-02,  2.3874e-01, -7.7305e-02,\n",
       "          -1.3435e-01, -3.4221e-01, -5.4182e-02,  2.7540e-01,  7.6374e-02,\n",
       "          -9.2565e-01, -5.4589e-01,  1.9456e-01, -4.1175e-01, -4.1072e-01,\n",
       "          -1.1538e-03, -4.6924e-01,  7.4291e-01,  2.4933e-01,  3.0939e-01,\n",
       "           3.2340e-01,  6.2222e-01, -7.4617e-01, -5.4213e-01, -5.9395e-01,\n",
       "          -4.6336e-01, -3.6726e-02,  2.6440e-01,  5.1142e-02, -5.5055e-01,\n",
       "          -4.4228e-01, -6.6341e-01, -6.6074e-02,  2.7430e-01, -7.1409e-01,\n",
       "          -8.9123e-01,  1.4332e-01, -5.8573e-02, -6.7373e-01, -9.3391e-01,\n",
       "          -1.2956e-02,  3.7783e-01,  9.9518e-02, -7.2109e-01,  5.1604e-01,\n",
       "           1.5309e-01, -2.3670e-01,  8.6065e-01,  2.1550e-01, -6.7432e-01,\n",
       "           7.8040e-01,  2.0606e-01, -3.7993e-01, -3.0433e-01, -4.6119e-02,\n",
       "           6.7712e-01, -8.3217e-01,  3.8331e-01,  7.8321e-01,  3.2621e-01,\n",
       "          -3.9223e-01,  4.1397e-01, -1.9262e-01,  3.6924e-01,  6.0574e-01,\n",
       "           3.1908e-01,  1.7844e-01,  2.9499e-01,  5.5664e-01,  1.3392e-01,\n",
       "           6.4561e-02, -1.1362e+00,  4.4414e-01,  1.2819e-01, -8.1194e-01,\n",
       "          -2.7238e-01,  8.2982e-01, -6.8089e-01,  8.7358e-02,  6.1616e-01,\n",
       "           1.8843e-01, -1.5430e-01, -3.0495e-01,  3.0751e-02,  6.6099e-01,\n",
       "           6.5262e-01, -3.6680e-01,  9.9663e-01,  3.8860e-02,  5.6791e-01,\n",
       "           2.2736e-01,  3.4228e-01,  3.6814e-02, -6.6970e-01, -1.3032e+00,\n",
       "          -3.1147e-01, -1.0875e-01, -3.1545e-01, -9.2252e-01, -2.0878e-01,\n",
       "           3.9600e-01, -3.4587e-01, -1.1277e-01, -6.2824e-01, -1.2032e+00,\n",
       "           2.3035e-01,  1.0802e-01,  5.6773e-02,  3.6595e-01,  1.1010e-01,\n",
       "          -4.3072e-01, -3.3437e-01,  2.5635e-01,  2.8082e-01,  5.9105e-01,\n",
       "          -1.6842e-01, -1.3486e-01, -3.2498e-01,  9.4679e-01,  1.0773e+00,\n",
       "           1.6062e-01, -8.6186e-02, -1.9909e-01, -7.5203e-01, -7.7744e-01,\n",
       "          -4.4563e-01,  5.6926e-02, -1.8440e-01, -3.0360e-02,  4.7883e-01,\n",
       "          -3.4681e-01, -1.5087e-01, -9.4689e-01,  3.2552e-01, -5.6055e-01,\n",
       "          -8.5170e-01,  3.5166e-01, -5.4453e-01, -8.1442e-01, -1.4926e-01,\n",
       "           1.7773e-01,  1.3436e-01,  9.1467e-02, -4.3461e-02, -2.5368e-01,\n",
       "           3.0649e-01,  2.2270e-01, -5.6849e-01, -5.5726e-01, -4.2377e-01,\n",
       "           7.5256e-01, -6.3489e-01,  1.5475e-01,  2.1217e-01,  9.6678e-01,\n",
       "          -2.1490e-01,  6.6815e-01, -6.4589e-01,  1.2344e-01,  1.6883e-01,\n",
       "          -1.3011e-01,  1.6048e-02, -9.1069e-02, -2.3607e-01, -4.3385e-02,\n",
       "          -4.8463e-01,  3.4972e-01,  2.7518e-01,  2.0132e-01,  3.5023e-01,\n",
       "          -1.1873e-01, -7.1451e-02,  3.9386e-01,  4.7890e-01, -3.2591e-01,\n",
       "           5.7934e-01, -1.9762e-01,  5.6106e-01,  6.1304e-01,  4.4344e-02,\n",
       "          -1.8784e-01, -2.4767e-01,  8.7647e-02, -2.4450e-01,  6.5796e-01,\n",
       "          -3.4828e-02,  8.0672e-02, -4.4616e-03,  4.2215e-01,  2.4807e-01,\n",
       "           2.4437e-01, -1.6276e-01,  6.1730e-01,  3.9666e-01,  3.3250e-03,\n",
       "          -1.2332e-01, -1.7634e-01,  1.9095e-01,  5.6129e-01, -1.7975e-01,\n",
       "          -1.7673e-01, -7.3470e-02, -1.1885e-01, -4.0681e-01, -2.7213e-01,\n",
       "           2.2383e-01,  6.0983e-02,  2.2395e-01,  3.2436e-01,  1.1279e+00,\n",
       "           2.2088e-01, -6.9033e-01,  5.7353e-03, -9.4738e-01,  3.3932e-01,\n",
       "          -3.1369e-01, -2.0001e-01, -1.2537e-01, -8.0907e-01, -4.7684e-01,\n",
       "          -3.3707e-01, -5.7593e-01,  1.7739e-01,  1.0139e-01,  2.9367e-01,\n",
       "           1.6176e-01, -3.7545e-01,  3.0762e-01, -6.7137e-01,  7.6232e-01,\n",
       "           3.5576e-01, -5.3670e-01, -2.5514e-01,  4.4419e-02, -6.1740e-02,\n",
       "           1.5291e-01,  2.5311e-01, -3.9619e-02,  3.0111e-01,  1.7381e-01,\n",
       "           5.2959e-02,  5.2477e-01,  2.6090e-02, -2.7340e-01, -3.6535e-01,\n",
       "          -9.3697e-02,  7.8001e-02,  3.9428e-01,  4.7802e-01,  4.0046e-01,\n",
       "           2.8278e-01,  8.3779e-01, -9.6890e-01,  7.9529e-02,  5.6226e-01,\n",
       "           6.4038e-02,  5.2281e-01, -2.4688e-02,  2.1864e-01, -8.4234e-02,\n",
       "           2.5310e-01,  1.0568e-01, -1.1871e-01, -1.5997e-01,  4.5324e-01,\n",
       "          -2.3012e-01,  2.8435e-01,  3.3298e-02, -3.0196e-01,  8.2735e-04,\n",
       "           2.6320e-03,  2.6953e-01,  5.6445e-01,  5.6946e-01,  2.9124e-03,\n",
       "           6.3326e-02]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.8209e-01, -4.0425e-01, -8.1328e-02,  1.2040e-02, -1.5698e-01,\n",
       "           8.6200e-01,  4.5449e-01, -3.9208e-03, -2.9383e-01, -6.8071e-01,\n",
       "          -2.6302e-01,  1.9687e-01, -2.1340e-01, -4.3485e-01, -4.3173e-01,\n",
       "           5.3952e-02, -1.8079e-01,  4.7937e-01, -6.5098e-01, -3.8907e-01,\n",
       "           6.6861e-01, -2.8892e-01, -2.6308e-01, -4.6188e-02,  3.7445e-01,\n",
       "           5.3853e-02,  7.1876e-02, -2.7944e-01,  1.1581e-01,  1.5133e-01,\n",
       "           9.3969e-01,  2.3323e-01,  4.7251e-01, -8.7349e-02, -3.3780e-01,\n",
       "           3.8599e-01, -8.4873e-01,  6.4432e-02, -8.9679e-01,  7.6351e-01,\n",
       "           4.2108e-02, -1.0294e-01,  4.0039e-01,  2.9525e-01,  4.0180e-01,\n",
       "           7.6886e-01, -9.9592e-01, -2.4678e-01, -2.3153e-01,  8.9361e-02,\n",
       "          -4.9461e-01, -7.9851e-01,  2.9007e-01,  2.3913e-01, -5.3192e-02,\n",
       "           6.7334e-01, -2.9732e-01,  5.3648e-01,  9.8594e-02, -6.3563e-01,\n",
       "           4.4965e-01, -5.4489e-01,  2.9397e-01,  2.5551e-02, -4.4064e-01,\n",
       "           2.7998e-01, -6.6097e-01,  5.5022e-01,  7.6650e-01,  3.8548e-01,\n",
       "          -3.1777e-01,  4.0524e-02, -1.4899e-01,  1.1290e-01, -4.4665e-03,\n",
       "          -6.3250e-01, -1.9267e-01,  1.8655e-01,  4.2908e-01, -1.4983e-01,\n",
       "          -8.3523e-01,  9.5099e-02, -5.4932e-02,  4.6289e-01,  2.4383e-01,\n",
       "          -1.7447e-01,  5.3021e-01, -1.8243e-02, -9.3626e-03,  7.1115e-03,\n",
       "          -2.0945e-01, -4.9100e-01, -4.0670e-01,  3.3748e-01,  4.1842e-03,\n",
       "           6.0846e-01, -3.7373e-01, -2.3015e-01, -1.4188e-01, -9.5924e-02,\n",
       "          -5.2702e-01,  6.9340e-01,  7.8075e-01, -3.7491e-01, -1.5814e-01,\n",
       "           3.9731e-01, -3.0142e-01, -3.7866e-01, -4.1647e-01, -4.0250e-01,\n",
       "           5.9753e-01, -3.2219e-01, -3.3121e-01,  4.9383e-01, -1.6819e-01,\n",
       "           5.3222e-01, -8.1578e-01,  1.8470e-01,  7.9405e-01, -5.9474e-01,\n",
       "          -8.7353e-02,  2.6687e-01,  1.7078e-01, -8.3379e-02,  3.8493e-01,\n",
       "           2.7839e-02, -7.9913e-01,  8.5259e-01,  6.7150e-02,  3.2066e-01,\n",
       "          -3.6087e-01, -2.0758e-01,  5.2482e-01, -1.5996e-01,  1.7848e-01,\n",
       "          -1.1708e-01, -8.3933e-02,  2.0155e-01, -2.6406e-01,  7.4356e-01,\n",
       "          -3.6634e-01,  4.3526e-01, -2.0421e-01, -4.8576e-01, -5.7155e-01,\n",
       "          -2.6073e-02, -2.3489e-01,  3.5021e-02,  1.1286e+00, -1.2200e-01,\n",
       "           3.4427e-01,  4.1172e-01,  6.7847e-01, -8.8098e-01, -1.3676e-01,\n",
       "           5.0232e-01, -4.1818e-01,  8.0798e-01, -5.0395e-01, -7.0016e-02,\n",
       "          -7.1902e-01,  2.7827e-02, -5.7529e-02, -3.4538e-01,  3.3593e-01,\n",
       "          -1.3653e-01, -7.1972e-01, -1.2974e-01,  1.6776e-01,  5.4240e-01,\n",
       "          -1.2129e-01, -8.0632e-01, -2.4511e-01,  5.5771e-01, -1.2786e-01,\n",
       "          -7.7923e-01, -4.0201e-01,  2.7001e-01,  2.7304e-01,  3.1408e-01,\n",
       "          -3.0107e-01,  2.1444e-01,  1.1322e-01, -6.5912e-01,  5.7789e-01,\n",
       "           5.6036e-02, -3.6189e-04,  5.9801e-01, -7.2198e-01,  9.7584e-02,\n",
       "          -4.8863e-01,  5.4155e-01,  2.1641e-01, -1.8268e-01, -6.1438e-01,\n",
       "          -9.1089e-02, -5.9295e-02, -9.4982e-02, -9.4091e-02, -1.1325e-01,\n",
       "           5.9777e-02,  3.4914e-01, -6.2158e-03,  6.7002e-01,  6.9476e-02,\n",
       "           6.6357e-02, -2.2184e-01, -2.8940e-01, -1.5000e-01,  1.3222e-01,\n",
       "           8.3212e-02, -2.8761e-01, -3.9809e-01, -5.4638e-01,  4.0356e-03,\n",
       "           6.4729e-01, -8.9142e-02, -9.4544e-02,  9.3540e-01, -7.1052e-01,\n",
       "           7.6038e-01, -2.4049e-01, -2.0526e-01, -1.2435e-02, -1.5465e-01,\n",
       "          -4.4841e-02, -9.4706e-02, -1.0276e-01,  2.0582e-01, -2.4202e-01,\n",
       "           1.3659e-01, -1.7015e-01,  1.0913e+00,  2.3827e-01,  2.5257e-01,\n",
       "           4.5349e-01, -1.8577e-01, -3.7515e-01,  1.0030e+00,  2.1975e-01,\n",
       "           5.2618e-01,  4.7543e-01, -1.7436e-01,  3.3544e-01,  1.1978e-01,\n",
       "          -7.5410e-02,  3.0799e-01, -2.9537e-01,  4.3062e-01,  1.4905e-01,\n",
       "          -2.1348e-01, -2.8420e-01,  1.2112e-01, -1.9704e-01,  5.4337e-02,\n",
       "          -2.8079e-01, -4.4008e-01, -1.5116e-01,  3.1600e-01, -2.0636e-01,\n",
       "           5.0482e-02,  4.4588e-02, -5.1318e-02,  2.5012e-01,  2.3425e-01,\n",
       "          -7.5182e-01,  4.4927e-01, -6.1150e-01,  2.3592e-01,  5.6081e-01,\n",
       "          -5.1407e-01, -3.8419e-01,  5.9122e-01,  3.3052e-01,  4.8673e-01,\n",
       "          -1.5617e-01, -5.5810e-01,  2.7101e-01, -5.4624e-01, -1.1592e-01,\n",
       "           1.0617e-01, -4.5365e-01, -1.8064e-01,  6.9433e-01,  2.5349e-01,\n",
       "          -5.6811e-02,  5.1531e-01,  8.8392e-02, -1.5837e-02,  1.2367e-01,\n",
       "          -2.5716e-01,  3.8112e-01,  7.9394e-01, -6.4049e-02, -2.3072e-01,\n",
       "          -1.2473e-01, -1.1944e-01, -1.3486e-01, -1.7640e-01,  3.5437e-01,\n",
       "          -4.9950e-01, -5.1782e-02, -1.9483e-01,  9.3216e-01,  3.3379e-01,\n",
       "          -3.7670e-01,  6.0984e-01, -2.9200e-01,  1.6852e-01,  3.1167e-01,\n",
       "          -2.6537e-01, -7.7595e-01, -6.7566e-01, -1.8255e-01,  1.5577e-01,\n",
       "          -7.5335e-02,  3.6950e-01, -9.5799e-01, -3.1445e-01, -8.7520e-02,\n",
       "          -5.0308e-01,  3.5433e-01, -1.7994e-01,  5.4342e-02, -3.2791e-01,\n",
       "           5.5025e-02,  9.0519e-02,  3.3170e-01,  5.7609e-01, -2.2157e-02,\n",
       "          -2.7110e-01, -1.1339e-01,  1.8846e-01,  1.5150e-02,  6.8529e-01,\n",
       "          -2.7111e-01,  3.0497e-01, -4.7668e-01,  3.3425e-01, -1.1193e+00,\n",
       "          -5.9045e-01,  1.5769e-01, -9.8506e-01,  1.2826e-01,  5.2836e-01,\n",
       "          -5.5924e-02, -8.0282e-02,  3.3470e-01,  3.5233e-01, -1.1297e-03,\n",
       "           3.9463e-01, -1.5356e-01,  2.4376e-01,  5.7466e-01, -1.0609e-01,\n",
       "          -6.0247e-01,  2.1985e-01, -1.2803e-01,  1.2934e-01,  3.4399e-01,\n",
       "           1.0843e-01,  3.0288e-01, -4.0927e-02, -4.6396e-01, -1.1999e-01,\n",
       "           5.7543e-01,  2.7812e-01,  3.5071e-01, -1.1442e-01,  9.0578e-01,\n",
       "          -1.0849e-01, -1.7271e-01, -5.7359e-01,  4.3939e-01,  1.7715e-01,\n",
       "          -2.0102e-01,  2.0772e-01,  4.3607e-01, -3.3781e-01, -4.6383e-02,\n",
       "          -2.2449e-01,  4.4254e-01,  3.5911e-01, -3.2378e-01, -1.2927e-01,\n",
       "           4.9745e-01, -9.0092e-02, -3.5693e-01,  9.5091e-02, -2.8478e-01,\n",
       "           3.8107e-02, -2.3451e-01, -5.7477e-01,  2.0915e-01, -3.4107e-01,\n",
       "          -2.8829e-01, -3.7412e-01, -6.5802e-01, -4.3875e-01, -3.9937e-02,\n",
       "           1.1389e-01, -1.8906e-01,  1.0588e+00,  3.5162e-01,  7.4184e-02,\n",
       "          -2.5943e-01, -3.9783e-01,  3.0142e-01, -4.8801e-01, -2.3728e-01,\n",
       "           5.2257e-01,  9.8094e-02,  2.1822e-01, -2.5993e-02,  1.3775e-01,\n",
       "           4.1747e-01,  1.6251e-02, -9.5507e-02, -2.9428e-01, -6.4210e-02,\n",
       "           1.7943e-01, -6.1566e-01,  5.7045e-01,  1.4581e-01,  2.9155e-01,\n",
       "           6.9704e-02, -6.1408e-01, -2.5210e-01, -5.0018e-01, -5.3970e-01,\n",
       "           2.2959e-01, -3.2175e-01, -1.6985e-01, -2.1661e-01,  5.0437e-01,\n",
       "          -3.6601e-01, -6.4275e-02, -5.8260e-01, -2.1839e-01, -1.2509e-01,\n",
       "           2.2231e-01,  7.0499e-01,  6.4209e-02,  8.3764e-02,  1.6259e-01,\n",
       "           1.1523e-01,  3.0256e-01, -3.1134e-01,  3.5375e-01, -1.2469e-01,\n",
       "          -1.6610e-01,  1.0978e-01,  6.3956e-01, -5.2632e-01,  1.4849e-01,\n",
       "          -6.3837e-02, -1.5302e-01,  2.0685e-01,  3.1986e-01,  6.7188e-01,\n",
       "          -7.9808e-01, -5.9629e-01, -1.6728e-01,  7.2175e-01, -2.0386e-01,\n",
       "          -2.7580e-01, -2.0766e-02,  2.5458e-01,  3.7073e-01, -6.1430e-02,\n",
       "          -3.6349e-01, -5.4440e-01,  8.0297e-01,  3.4189e-01,  1.5344e-01,\n",
       "           2.3803e-01, -2.0059e-02, -1.7845e-01,  9.2193e-02,  1.8712e-01,\n",
       "           1.4282e-01, -6.6099e-01,  1.4343e-01,  2.9872e-02, -1.6911e-03,\n",
       "           2.2599e-02,  1.6898e-01, -2.3829e-01, -1.9503e-01, -4.5624e-01,\n",
       "           9.2993e-02, -9.2559e-01, -1.7448e-01, -2.3216e-01,  8.8090e-01,\n",
       "          -6.0901e-01, -1.7101e-01, -2.3918e-01,  7.5650e-01,  1.1504e-01,\n",
       "           3.0901e-01, -9.3486e-01, -2.1445e-01,  1.5048e-01,  7.0300e-01,\n",
       "           3.4146e-01,  1.6370e-01, -8.1076e-02,  6.9670e-02, -1.3860e-01,\n",
       "          -4.0044e-01,  5.1144e-01,  5.4275e-01, -5.6066e-01,  2.4634e-01,\n",
       "           1.9954e-01, -5.2503e-01,  7.4333e-02, -3.0682e-01, -6.0531e-01,\n",
       "           2.4245e-01, -1.4376e-01,  2.2112e-01,  6.1187e-01, -1.7237e-01,\n",
       "           1.1802e-01,  4.1010e-01,  7.1419e-01,  2.7187e-01, -2.8533e-01,\n",
       "          -2.4334e-01,  1.7848e-01, -4.5396e-02, -8.3116e-01, -3.4033e-01,\n",
       "           4.6569e-01,  1.9452e-01, -1.4059e-01, -2.7698e-01, -1.1674e+00,\n",
       "          -4.6165e-02,  1.2859e-01,  3.1541e-01,  3.2527e-01,  2.4309e-01,\n",
       "           1.1561e-01,  3.7484e-01,  1.2131e-01,  2.4409e-01, -4.3084e-01,\n",
       "           5.0411e-01, -2.1522e-01, -2.2337e-01,  7.2958e-02,  3.6163e-01,\n",
       "           2.2069e-01, -2.0307e-01, -2.6502e-02,  1.6190e-01,  5.2953e-01,\n",
       "          -4.2349e-02, -3.9015e-01,  9.4775e-01,  3.9475e-01,  2.7201e-01,\n",
       "           7.0285e-01,  2.0270e-01,  3.0596e-02,  1.0724e-02,  9.4189e-02,\n",
       "           8.9129e-01, -2.1565e-01,  4.0763e-01,  5.9356e-01, -3.3243e-01,\n",
       "           2.2796e-01,  6.2407e-02, -5.8728e-01,  1.2454e-01, -2.2971e-01,\n",
       "           3.6650e-01,  1.2876e-01, -5.2020e-02,  6.5446e-02,  4.0183e-02,\n",
       "           4.6876e-01, -1.2546e-02, -7.1406e-01,  5.2701e-01, -1.0073e-01,\n",
       "           4.1636e-01, -7.4091e-01, -9.0802e-01, -2.0719e-01,  5.7946e-02,\n",
       "           6.1346e-01,  7.1585e-01,  2.2351e-01,  8.9303e-01, -2.4784e-01,\n",
       "          -7.2494e-02,  7.2174e-02, -5.8901e-01, -2.5333e-01,  7.6537e-02,\n",
       "           2.0935e-01, -4.5398e-01,  4.5002e-03, -6.3866e-02, -1.1160e-02,\n",
       "           3.3572e-01, -6.1080e-01,  2.1973e-01,  4.8655e-01, -1.1382e-01,\n",
       "           9.2366e-02, -2.0267e-01, -1.7418e-02,  6.5815e-01, -4.8753e-02,\n",
       "          -7.1141e-01,  7.0936e-02,  3.2154e-01,  2.3180e-01,  3.6889e-01,\n",
       "          -5.5864e-01,  4.8054e-01, -4.7654e-01,  3.9696e-01,  3.6323e-01,\n",
       "           2.3116e-01,  1.6543e-01, -2.1661e-01,  9.5978e-01,  4.0312e-01,\n",
       "           6.1294e-01,  1.9830e-01, -7.8525e-01,  4.6587e-01,  5.8036e-01,\n",
       "          -1.8636e-01, -3.0818e-01, -2.2050e-01,  2.9842e-01, -6.0609e-01,\n",
       "           5.3495e-01, -8.6639e-01,  9.8827e-02,  3.8318e-01, -3.7081e-01,\n",
       "          -9.1043e-01, -8.4295e-01, -1.3441e-02,  5.7609e-01,  7.5536e-01,\n",
       "          -6.4383e-01,  2.3739e-01,  1.8799e-01, -2.4409e-01, -7.2365e-01,\n",
       "           4.6243e-02, -1.1712e-01, -2.4771e-01, -5.6372e-01, -4.8688e-01,\n",
       "          -4.4703e-02,  9.2276e-01,  3.3230e-01,  2.7671e-01,  6.4065e-02,\n",
       "           1.1889e-01, -4.3419e-01,  2.7148e-01, -6.7659e-02, -4.7382e-02,\n",
       "          -3.5852e-01, -7.3968e-02,  4.5317e-02, -4.8489e-01,  8.6596e-01,\n",
       "           6.7942e-01, -3.3902e-01, -8.1937e-01,  1.4110e-01,  4.0188e-02,\n",
       "           4.3932e-01,  2.1532e-01, -1.9068e-01, -2.4219e-01,  2.5121e-02,\n",
       "           4.2302e-02, -5.9432e-01, -1.3703e-01,  4.1256e-01, -1.1519e+00,\n",
       "          -4.3968e-01, -3.5344e-01, -3.2021e-01, -1.2287e-01,  5.0276e-01,\n",
       "          -6.7355e-01, -6.5542e-01,  7.8478e-01, -8.0749e-01,  2.0192e-01,\n",
       "          -9.5226e-01,  3.2092e-01,  1.6693e-01, -3.4130e-01,  1.5080e-01,\n",
       "          -1.3962e-01,  4.0581e-01,  1.6328e-01,  1.4636e-01,  2.1925e-01,\n",
       "           3.6179e-01, -3.9569e-01,  5.0742e-01,  7.5153e-01,  8.6790e-01,\n",
       "          -4.7952e-01, -9.5336e-03, -4.7532e-01, -4.6548e-01,  1.0031e+00,\n",
       "          -3.9114e-01,  3.2549e-01, -1.8144e-01,  7.6524e-01,  1.3804e-01,\n",
       "          -6.8963e-01,  6.3040e-02,  1.8359e-01,  1.4621e-01, -1.9961e-01,\n",
       "           1.8418e-01, -1.8567e-01,  3.4048e-01,  8.8622e-02,  1.2539e-01,\n",
       "          -6.0412e-01,  2.1190e-01, -6.9903e-03, -5.4608e-01, -4.2381e-01,\n",
       "           9.1446e-02, -8.6741e-01,  1.2006e-01, -4.2564e-01, -2.3805e-01,\n",
       "          -2.6292e-01]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.1909e+00,  7.1004e-01, -2.8457e-01, -1.9890e-01,  7.1491e-02,\n",
       "          -3.0790e-01,  1.1833e+00, -8.7267e-01,  8.2911e-01,  1.0980e+00,\n",
       "          -3.2780e-01,  2.2221e-01, -2.9218e-01, -5.1404e-01,  4.4448e-01,\n",
       "          -4.3593e-01, -3.2896e-01,  1.1520e+00,  1.4676e-01, -8.6871e-01,\n",
       "           1.0852e+00,  8.0755e-02,  2.0601e-01, -2.1112e-01, -2.3614e-02,\n",
       "           1.8841e-01, -7.3049e-01, -4.2354e-01,  2.1807e-01,  1.2645e-02,\n",
       "          -4.7872e-01, -9.2741e-02, -6.2272e-01,  1.4941e-01,  6.1521e-01,\n",
       "          -3.0749e-01, -8.1713e-01,  2.8805e-01,  3.0047e-01, -6.7070e-01,\n",
       "          -3.7338e-01, -3.2499e-01, -6.1259e-02, -3.6592e-02,  2.0585e-03,\n",
       "          -5.0095e-01,  1.4161e-01,  4.4852e-01, -1.3907e-01, -1.5664e-01,\n",
       "          -9.1638e-01, -8.6154e-01, -2.7300e-01,  8.9656e-03, -3.1560e-01,\n",
       "          -5.9975e-01, -1.3427e-01, -2.3490e-01,  1.0065e+00,  3.9431e-01,\n",
       "           1.8137e-01,  3.3487e-02,  2.4992e-01,  2.1080e-01, -5.3495e-02,\n",
       "          -1.3573e-01, -4.0693e-01, -6.6502e-02,  9.0271e-02, -8.3092e-01,\n",
       "          -4.5652e-01,  5.4964e-01,  7.1114e-01, -6.7099e-02,  5.6642e-02,\n",
       "           3.9347e-01, -8.8921e-01,  2.6606e-01, -5.5776e-01, -1.2496e-01,\n",
       "          -4.5306e-01, -2.6799e-02, -5.8802e-01,  1.1975e-01,  3.9904e-01,\n",
       "           1.1081e-01,  1.9218e-01,  2.9064e-01,  8.7726e-02, -7.1898e-01,\n",
       "           4.6611e-01,  5.9936e-01,  7.0988e-01, -4.4844e-01, -5.6124e-01,\n",
       "          -9.3333e-02, -8.3500e-01, -3.5865e-01,  2.9865e-01,  1.6322e-01,\n",
       "          -4.6166e-01, -1.8169e-01,  4.9937e-01,  9.5244e-01,  2.1600e-01,\n",
       "          -2.4646e-01, -6.4579e-01, -4.9834e-01,  5.8650e-01, -7.7322e-01,\n",
       "           8.4448e-01, -1.0401e+00, -6.2691e-01,  2.7014e-01,  1.0695e-01,\n",
       "           5.2808e-01,  1.1148e+00, -2.0156e-01, -8.4551e-01,  6.2778e-02,\n",
       "           7.1110e-01,  3.8050e-01,  3.5811e-01,  4.1775e-01,  1.5058e+00,\n",
       "          -1.6615e-03,  9.2123e-01,  8.1788e-02, -4.3213e-01,  5.5627e-01,\n",
       "           6.6339e-02, -1.5589e-02, -8.2734e-02, -3.2168e-01, -3.9556e-01,\n",
       "          -1.2448e-01,  9.3217e-02, -3.8692e-01, -4.6339e-01,  7.5772e-01,\n",
       "           5.9329e-01, -3.5601e-01, -3.5524e-02,  4.6571e-01,  7.1221e-02,\n",
       "           6.5674e-01,  3.6730e-01, -1.9440e-01,  1.7993e-01,  1.3468e-01,\n",
       "           6.2306e-01,  1.5712e-01,  6.2573e-01,  6.3277e-01,  1.3726e+00,\n",
       "           1.5737e+00,  6.6304e-01, -6.4636e-01, -2.2788e-01, -1.9624e-01,\n",
       "           3.5378e-01,  1.5645e-02,  7.1222e-01, -3.9688e-01,  2.9283e-01,\n",
       "           3.2380e-01,  2.2887e-01, -5.1873e-01, -3.0390e-01,  2.7336e-01,\n",
       "           1.0851e+00, -6.8701e-01, -6.9111e-02, -6.8451e-02,  4.7656e-01,\n",
       "           6.5490e-01, -8.6322e-02,  4.3067e-01, -7.9712e-01, -1.3158e+00,\n",
       "          -2.4580e-01,  2.7303e-01,  1.0266e-01,  4.7706e-01, -4.5991e-01,\n",
       "           4.9561e-01,  4.4913e-01, -7.1057e-01,  5.9488e-01,  3.2774e-01,\n",
       "          -4.1176e-01, -1.5395e-01,  9.3603e-01,  3.0888e-02,  2.7702e-01,\n",
       "          -5.4602e-01, -7.7087e-01, -5.7351e-01, -2.5481e-01, -7.0623e-01,\n",
       "          -2.3934e-01, -4.9797e-01,  6.8965e-02, -3.0262e-01,  3.3468e-01,\n",
       "          -3.0870e-01,  4.5894e-01,  3.3117e-01, -2.5773e-02, -4.4405e-02,\n",
       "          -1.0214e-02, -6.2902e-01, -8.0999e-02, -3.4005e-01, -8.6788e-01,\n",
       "           5.6819e-02, -4.1034e-01,  1.0537e+00,  6.3242e-01, -5.1652e-01,\n",
       "          -1.5385e-01, -4.6287e-01, -4.9263e-01, -1.7945e-01, -2.9041e-01,\n",
       "          -8.4487e-01,  1.7028e-01,  1.1639e+00, -5.2355e-01,  1.9786e-01,\n",
       "          -4.4232e-01,  6.8613e-01,  4.4723e-01,  8.1680e-01,  4.9435e-02,\n",
       "          -7.0147e-01, -4.8536e-01, -3.4860e-02, -6.1359e-01, -5.8874e-01,\n",
       "          -2.8797e-01, -3.1975e-01,  3.3494e-02, -6.8307e-02,  2.5821e-01,\n",
       "          -8.8794e-01, -6.3728e-01,  1.1237e+00,  1.2107e-01,  7.2636e-01,\n",
       "          -4.3888e-01, -3.8328e-01,  2.7507e-01,  7.5338e-02, -8.3978e-01,\n",
       "          -4.0318e-01, -8.9994e-01, -9.7178e-01, -5.6414e-01, -2.1454e-01,\n",
       "          -3.7992e-01,  7.0003e-01,  5.2093e-01,  8.4035e-01,  2.7294e-01,\n",
       "           3.5074e-01, -2.0822e-01,  5.6888e-02,  6.4932e-01, -2.5003e-01,\n",
       "          -2.7281e-01,  4.9479e-01, -3.8106e-01, -2.8573e-01, -5.9013e-01,\n",
       "          -3.0722e-01, -1.8654e-01,  5.0137e-01,  5.4475e-02,  1.7372e-01,\n",
       "           6.8242e-01,  2.6045e-01,  6.5108e-01,  5.8688e-01,  8.9296e-01,\n",
       "          -6.7523e-01,  9.6378e-01, -2.3793e-01, -4.4856e-01, -3.1029e-01,\n",
       "          -3.6658e-01, -2.1638e-01,  1.5333e-01, -5.0437e-01,  1.1366e+00,\n",
       "           7.3430e-01, -1.8417e-01, -5.5644e-01, -3.5600e-01,  5.4520e-01,\n",
       "           2.6163e-01, -1.3380e+00,  6.1044e-01,  4.9697e-01,  4.7378e-01,\n",
       "           1.7973e-01, -8.4518e-01,  2.3999e-01, -1.7831e-01, -8.8068e-01,\n",
       "          -6.2842e-01, -1.1549e-01, -5.0444e-01, -1.4439e-01,  6.4334e-01,\n",
       "           5.4361e-01, -4.0697e-01,  2.4283e-01, -7.7937e-01,  5.3102e-01,\n",
       "          -4.0358e-01,  2.4536e-01,  3.2139e-01,  2.4272e-01,  6.7587e-02,\n",
       "           1.6882e-01, -7.6103e-02, -2.2683e-01,  8.3451e-01,  3.2599e-01,\n",
       "           6.8877e-02,  8.9984e-01, -7.7746e-02,  6.3158e-01,  3.9701e-01,\n",
       "          -1.9226e-01, -5.1975e-01,  6.3224e-01, -1.0128e-01,  3.1393e-01,\n",
       "          -2.4428e-01, -3.8531e-01, -7.0274e-01,  1.0293e-01,  3.8112e-01,\n",
       "           1.9029e-01,  6.7784e-01,  7.5989e-02, -7.8017e-01, -1.0177e-01,\n",
       "           3.3777e-01, -6.2193e-01,  6.5087e-01, -1.1214e+00,  4.6059e-01,\n",
       "           2.9532e-01,  8.3832e-02, -2.4258e-01, -3.7912e-01,  2.1827e-01,\n",
       "           3.7101e-02,  5.0491e-01,  3.1876e-01, -9.8048e-01, -1.5094e-01,\n",
       "           1.1584e+00, -4.0988e-01,  6.2193e-01,  5.0562e-01,  6.0871e-02,\n",
       "           9.4080e-01,  2.9751e-01,  2.3648e-01,  1.5372e-01, -6.6805e-01,\n",
       "          -3.2227e-01, -5.1185e-01,  3.5537e-01,  1.4598e-01, -1.1095e-02,\n",
       "          -1.4264e-01,  4.8001e-01, -6.4151e-01, -3.5502e-01,  3.2812e-01,\n",
       "           1.3403e-03,  6.3876e-01, -4.9190e-01, -4.0688e-02, -3.9321e-01,\n",
       "           1.7058e-01, -3.6406e-01,  7.8583e-01,  5.7630e-01,  9.9213e-02,\n",
       "           1.8874e-01, -6.2073e-02, -1.6143e-02, -3.0322e-01, -3.0727e-01,\n",
       "           2.6624e-01, -6.0255e-01,  9.4717e-01, -4.3492e-01, -7.8383e-01,\n",
       "          -6.7331e-01,  1.0211e+00, -4.0575e-01, -4.2052e-01,  5.0935e-02,\n",
       "          -6.0992e-02,  2.9615e-01,  7.5892e-01,  2.0234e-01,  2.5282e-01,\n",
       "           3.7673e-01,  1.8196e-01,  3.8863e-01, -1.4816e-01, -4.4773e-01,\n",
       "          -1.7235e-01,  5.1348e-01, -4.3759e-01, -3.6428e-01, -2.7971e-01,\n",
       "          -3.4957e-02, -2.4096e-01, -6.8900e-01,  3.9922e-03,  2.3792e-01,\n",
       "           7.8930e-01, -9.5882e-01,  1.4299e-01,  4.5041e-01, -3.4951e-01,\n",
       "           1.7542e-01,  2.6330e-01, -1.4688e-01, -2.2854e-01, -5.2785e-01,\n",
       "           1.5683e-01,  9.6679e-03, -3.8899e-01, -4.2613e-01, -1.0855e+00,\n",
       "           8.4257e-02,  1.5503e+00,  6.8321e-01, -7.6661e-02, -6.2905e-01,\n",
       "           1.3022e-01,  9.1999e-01, -6.6478e-01,  1.3249e-01, -4.2248e-01,\n",
       "          -6.3164e-02, -4.2992e-01, -5.3854e-02, -1.4316e-01,  4.2632e-01,\n",
       "           5.9836e-01,  8.3092e-02,  1.4919e-01,  3.5329e-01,  1.0716e-01,\n",
       "          -5.4561e-02, -1.2879e+00,  1.5366e-01, -1.9227e-01, -6.6876e-01,\n",
       "           8.0987e-01, -5.1756e-02, -4.0315e-01, -2.9821e-01,  3.2271e-01,\n",
       "          -5.9485e-01,  4.4230e-01, -1.1343e-01, -7.2389e-01,  4.0858e-02,\n",
       "          -5.3998e-01, -4.4651e-01,  4.7015e-01,  7.5743e-02, -6.4086e-01,\n",
       "          -4.6411e-01,  3.1482e-01,  3.4552e-01,  2.0552e-01, -1.8875e-01,\n",
       "          -3.6616e-01,  2.0430e-01,  1.1481e+00, -5.3907e-01, -4.3311e-01,\n",
       "           4.9230e-01,  8.2873e-01, -2.8653e-01, -3.7682e-01, -6.3471e-01,\n",
       "           1.1124e+00,  8.9225e-01,  1.9170e-01,  5.9083e-01,  1.6004e-02,\n",
       "           3.5951e-01,  2.8068e-01,  2.1624e-01,  9.2875e-02,  1.0893e+00,\n",
       "          -4.0859e-01,  4.0216e-01, -1.4324e-01, -5.7749e-01, -2.8097e-01,\n",
       "           4.1848e-01,  4.0102e-01,  6.9210e-01, -3.3357e-01,  1.4995e-01,\n",
       "          -4.5917e-01,  1.2014e-01,  3.8461e-01,  4.9513e-01, -9.4641e-01,\n",
       "          -2.8480e-01,  8.6144e-02, -3.3111e-03,  3.1255e-01,  3.0944e-01,\n",
       "          -5.6675e-01, -4.5777e-01, -5.8452e-01,  8.1105e-01,  7.4171e-02,\n",
       "           1.0522e-02, -6.2158e-01, -4.5758e-01,  5.1957e-01,  2.4547e-01,\n",
       "          -4.9171e-01, -4.9269e-01, -5.7585e-01, -1.8198e-01,  1.5087e-01,\n",
       "          -3.3947e-01,  3.7854e-01,  6.5054e-01,  4.6438e-01,  7.6244e-01,\n",
       "           2.8406e-01,  1.3327e-01, -3.4538e-02,  8.0099e-01, -2.0909e-01,\n",
       "           1.0488e+00,  6.6658e-01,  3.9006e-01, -4.5878e-01,  1.3096e-01,\n",
       "          -3.1475e-01, -2.6521e-02, -7.7600e-02,  6.9184e-01,  4.9401e-01,\n",
       "          -6.2429e-01,  9.6145e-02, -7.5386e-01, -1.8885e-01, -5.6054e-01,\n",
       "          -3.2447e-01,  5.5167e-01, -3.1513e-01, -1.2226e-01,  5.9274e-01,\n",
       "           7.8492e-01,  3.0354e-01, -9.2351e-01,  2.6414e-01,  4.1658e-01,\n",
       "          -5.4593e-01, -4.5202e-01,  4.0778e-01, -9.9360e-01,  4.2312e-01,\n",
       "          -4.0318e-01,  1.1240e+00, -8.0326e-01,  3.1027e-01,  3.3827e-02,\n",
       "           3.9260e-01, -2.7242e-01,  9.8453e-01,  8.0492e-02,  1.1104e-01,\n",
       "           4.4484e-01, -2.6583e-01,  2.3292e-01, -2.3824e-01,  7.1523e-01,\n",
       "           5.1746e-02,  8.2095e-01, -7.6216e-01,  4.9556e-01,  2.2042e-01,\n",
       "           3.7443e-01,  4.3111e-01,  1.0052e+00, -1.2096e-01, -8.5727e-01,\n",
       "          -8.1680e-01,  2.2946e-01, -1.3218e-01, -2.8565e-01, -3.9809e-01,\n",
       "          -1.3499e-01, -9.4854e-02, -2.3995e-01,  3.1440e-01,  8.7385e-01,\n",
       "          -4.1566e-01,  2.3092e-01,  3.1843e-01,  2.0061e-01, -9.6075e-01,\n",
       "           9.7880e-02,  3.0867e-01,  7.0836e-01, -3.9830e-01, -1.5469e-01,\n",
       "           3.7800e-01, -1.3198e-01, -4.1789e-01,  2.1934e-01,  8.2962e-02,\n",
       "           3.7549e-01, -1.4938e-01, -1.5758e-01,  9.0220e-01, -1.6439e-01,\n",
       "          -7.5842e-01,  2.5556e-01,  6.6857e-02,  1.0360e-01, -5.6415e-01,\n",
       "          -2.8190e-01,  4.4024e-01,  2.5692e-01, -1.1837e-01,  6.4824e-02,\n",
       "          -3.2617e-01, -5.1704e-01, -8.9209e-01,  7.2835e-02,  9.6489e-02,\n",
       "           2.7551e-01,  3.5209e-01,  3.5302e-01, -4.0607e-01, -7.9962e-01,\n",
       "           1.1669e-01, -6.5510e-02,  5.8583e-01, -1.8656e-01, -9.1800e-01,\n",
       "          -2.1025e-01,  9.5476e-02, -9.4981e-01, -8.2028e-01, -1.1856e+00,\n",
       "           1.2107e-01, -8.0750e-01, -6.4723e-01,  1.3270e-01, -8.4254e-01,\n",
       "          -1.1896e-01, -1.4847e-01,  6.5831e-01,  6.5828e-01, -4.2275e-02,\n",
       "           1.7751e-02, -4.1857e-01, -7.6999e-02,  4.4782e-01, -1.4237e-01,\n",
       "           4.6138e-01,  1.3097e-01, -8.2445e-01,  4.4841e-01,  4.1268e-01,\n",
       "          -7.8308e-01,  2.2138e-01,  7.5166e-01,  7.5112e-02,  5.0148e-01,\n",
       "          -3.3557e-02,  6.5352e-02,  5.1220e-01, -2.3137e-02, -9.7284e-01,\n",
       "          -1.2324e+00, -4.3324e-01, -9.2393e-01, -2.4089e-02, -6.3108e-01,\n",
       "          -6.0932e-01,  1.8950e-01,  7.2056e-01,  1.6361e-01,  7.4552e-02,\n",
       "          -7.2233e-02, -4.6059e-01,  1.3321e-01,  3.3574e-01, -9.6003e-01,\n",
       "           5.8806e-01,  6.1880e-01,  2.2866e-01,  2.2749e-01,  4.5991e-01,\n",
       "          -2.8035e-01,  1.0977e-01, -1.4489e+00, -1.4717e-01,  3.1344e-01,\n",
       "           3.3687e-01,  1.1013e-01,  2.4320e-01, -5.2547e-01,  1.0272e-01,\n",
       "           1.4140e-02, -2.4560e-01, -5.4686e-01, -4.0880e-01, -7.8279e-01,\n",
       "           3.5470e-01,  1.9253e-02,  8.9307e-01,  2.2242e-01, -9.7166e-01,\n",
       "          -9.1016e-01,  9.1861e-01,  1.2857e-01, -2.3805e-01,  5.2801e-02,\n",
       "           2.3975e-01,  2.0274e-01,  9.1497e-02, -3.0618e-01,  3.4495e-01,\n",
       "          -5.1700e-01]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-1.6588e-01,  1.0734e-01,  1.8809e-01, -5.2424e-01,  4.3383e-01,\n",
       "           9.5771e-02, -1.7160e-01,  2.8883e-01,  3.1836e-01,  7.1986e-02,\n",
       "           1.6960e-01,  4.6794e-01,  6.0665e-01, -4.2604e-02, -4.7828e-01,\n",
       "          -4.0295e-04, -9.4287e-01,  7.1718e-03,  3.7177e-02,  5.5908e-02,\n",
       "          -3.7290e-01, -1.4232e-03, -2.2287e-01, -3.7841e-02,  2.5854e-01,\n",
       "          -1.7225e-02,  3.0272e-01, -2.9720e-01,  4.4470e-01,  3.9412e-01,\n",
       "          -5.3976e-01, -4.1755e-01,  1.6165e-01,  2.0131e-01,  2.0139e-01,\n",
       "          -5.5571e-01,  1.0656e-01, -1.0302e+00, -7.3909e-01, -1.4551e-01,\n",
       "           1.0339e-01,  8.2175e-02,  3.2972e-02,  3.6695e-01,  9.8449e-01,\n",
       "          -8.5955e-02, -2.4001e-01,  2.9887e-01, -1.9642e-01, -4.4092e-01,\n",
       "          -2.1406e-01,  1.4469e-01, -4.0905e-01,  3.6965e-01,  3.8908e-01,\n",
       "          -3.6403e-01,  3.8270e-01,  6.3962e-01,  6.6138e-01, -9.7769e-01,\n",
       "          -7.7873e-01,  6.7037e-01, -3.7799e-01,  2.2157e-01, -4.6755e-01,\n",
       "           3.7905e-01,  6.3112e-01,  4.3536e-01, -1.3502e-01, -6.0867e-01,\n",
       "           4.5969e-01,  1.4343e-01,  5.2514e-01, -3.5479e-02,  1.0854e-01,\n",
       "          -2.4930e-01,  6.0103e-01,  3.5211e-01,  2.1282e-01, -3.6226e-02,\n",
       "           2.8837e-01,  1.2880e-01, -5.6448e-02, -3.9006e-01,  4.1840e-01,\n",
       "          -4.8897e-03,  1.0195e-01, -2.6781e-01,  5.2453e-01, -6.6861e-01,\n",
       "          -1.0496e-01,  1.8718e-01, -2.3793e-03,  3.2240e-01, -3.0513e-01,\n",
       "           4.3168e-01,  7.6757e-02,  4.2281e-02,  8.0803e-01, -6.7698e-01,\n",
       "           2.0868e-01,  3.0815e-01, -2.7085e-01,  1.5014e-01,  9.8912e-02,\n",
       "          -1.0653e-01,  1.5493e-01, -2.3863e-01, -4.3760e-01,  2.9118e-01,\n",
       "          -8.5479e-02, -5.4552e-01, -4.7795e-01,  5.0261e-01,  1.1845e-01,\n",
       "           4.2155e-01, -6.8354e-02, -3.0417e-01, -1.1353e-01, -2.6052e-01,\n",
       "           4.9223e-02,  1.2771e-01, -5.2579e-01,  5.0533e-01,  5.7649e-01,\n",
       "           1.2239e-01,  1.4628e-01, -4.9439e-01, -3.4688e-01,  5.4867e-01,\n",
       "          -1.8220e-01, -3.3685e-01,  2.9926e-01,  4.6295e-01,  4.3328e-02,\n",
       "          -1.4650e-01, -2.1695e-01,  1.2299e-02, -6.7812e-01,  2.9073e-01,\n",
       "          -2.1654e-01,  1.4159e-01,  3.4695e-01, -3.5611e-01, -2.3997e-01,\n",
       "           7.0041e-01,  4.1821e-01, -8.9769e-01, -1.6385e-01,  7.9768e-01,\n",
       "           1.0730e+00,  3.5788e-01, -5.5889e-01,  4.6515e-01,  2.3200e-01,\n",
       "          -9.2020e-01, -1.9182e-01,  1.4257e-01, -4.6426e-01,  2.8402e-01,\n",
       "          -2.2342e-01,  3.3807e-01, -1.1324e-01,  1.3264e-01,  6.7221e-01,\n",
       "           8.0115e-01,  3.2503e-01,  3.7499e-01,  4.3731e-01, -1.3119e-01,\n",
       "           2.3942e-01, -4.8904e-01, -6.3982e-01,  1.1445e-01, -5.8387e-02,\n",
       "           9.3328e-01,  2.4514e-01,  7.9977e-03,  1.0441e-01, -3.1821e-01,\n",
       "          -2.7864e-01,  5.0849e-01,  4.4515e-01, -7.0455e-01,  6.4878e-01,\n",
       "           7.3571e-01, -9.3949e-01, -9.4260e-01,  1.2699e-01, -5.3167e-01,\n",
       "           2.5513e-01, -8.0705e-02,  2.0845e-01,  1.4569e-01, -5.3001e-01,\n",
       "          -1.3693e-01, -7.6703e-02, -1.3834e-01,  6.4566e-01,  3.1716e-01,\n",
       "           3.9232e-01, -4.4793e-01,  5.1177e-01,  6.2370e-01,  4.0685e-01,\n",
       "          -3.8108e-01, -5.6261e-01,  7.5837e-01,  5.3307e-01,  2.1631e-01,\n",
       "           7.5956e-02,  8.7366e-01, -3.0807e-01,  3.6099e-01,  2.2007e-01,\n",
       "          -9.8102e-02, -6.4915e-02,  3.0256e-01, -7.6555e-01, -7.0855e-01,\n",
       "           1.4324e-01, -2.3963e-01, -1.7722e-02, -2.6311e-01,  4.4209e-01,\n",
       "          -1.1235e+00,  6.3410e-01,  9.5721e-02, -5.5930e-02,  6.2047e-02,\n",
       "          -8.8128e-02, -5.6306e-01,  2.4688e-01, -2.1869e-01, -1.1861e+00,\n",
       "          -5.7616e-01, -3.1093e-01, -3.8146e-02, -1.3349e-01,  2.5575e-01,\n",
       "          -2.6460e-01,  2.2958e-01, -2.9014e-01, -3.7284e-01,  3.3920e-01,\n",
       "          -3.4178e-01,  2.2852e-02,  7.5422e-02, -2.5709e-02,  6.3362e-01,\n",
       "           1.9672e-01,  7.0499e-02, -2.1432e-01, -5.6520e-01,  2.0674e-01,\n",
       "           2.5174e-01, -1.5000e-01, -2.7968e-01,  2.2347e-01, -7.8936e-01,\n",
       "           5.1800e-02,  1.3141e-01,  1.3623e-01, -4.9666e-01, -2.2877e-01,\n",
       "          -2.1471e-01,  9.4237e-02,  4.2667e-01, -1.9551e-01, -3.2428e-01,\n",
       "           2.6110e-01,  3.0850e-01, -8.3451e-02,  1.2783e-01,  5.5875e-02,\n",
       "           2.5442e-01,  5.7702e-01, -1.6392e-01, -5.8214e-01, -1.7504e-02,\n",
       "          -2.1415e-02,  1.4213e-01, -6.6063e-01,  3.2739e-01,  3.3009e-01,\n",
       "           4.6890e-01, -1.0160e-01, -2.4671e-01, -1.3415e-02,  1.8393e-01,\n",
       "          -1.9981e-01,  3.8689e-01, -5.4365e-01,  4.3339e-01,  3.8808e-01,\n",
       "           3.1425e-01, -3.0332e-02,  3.5088e-01, -5.8766e-02,  9.9386e-02,\n",
       "           4.0469e-02,  8.5265e-02, -8.4363e-01, -4.7979e-02, -3.8060e-02,\n",
       "           6.6590e-01,  9.2606e-01, -1.3827e-02,  1.1267e-01,  8.0740e-01,\n",
       "          -2.5102e-02, -1.6857e-01, -8.2912e-02,  2.0483e-01,  5.9797e-02,\n",
       "           7.2070e-02, -3.9897e-01, -1.7608e-01,  1.8912e-01, -5.1242e-01,\n",
       "           3.3668e-01,  1.1430e-01, -2.3751e-01,  5.4488e-01,  4.2678e-01,\n",
       "           3.8631e-01, -5.4216e-01, -9.1944e-01,  8.8154e-02, -2.1094e-02,\n",
       "          -2.9923e-01,  7.5837e-02,  2.7835e-01,  3.7922e-01,  5.8371e-01,\n",
       "          -2.4364e-02, -6.6036e-02, -1.6158e-01, -7.5993e-02, -6.8471e-02,\n",
       "          -2.2123e-01, -3.3338e-01,  4.8861e-02,  2.4462e-02,  6.8955e-01,\n",
       "           4.9027e-02,  8.4622e-02, -3.2744e-01,  7.9622e-02, -5.7898e-01,\n",
       "          -8.3755e-01, -3.5257e-01,  2.8235e-01,  3.8205e-01,  3.0256e-02,\n",
       "          -6.7287e-01,  7.4121e-01, -2.7943e-01,  5.3338e-01, -1.2230e-01,\n",
       "           8.1317e-02,  7.3513e-01, -8.2706e-02,  5.4312e-02,  3.6592e-01,\n",
       "          -2.5014e-01, -3.0615e-01, -1.3693e-01,  2.6736e-01, -3.3487e-01,\n",
       "           4.2994e-01, -1.5100e-01,  1.0800e-01,  4.6111e-01,  1.0925e-01,\n",
       "          -1.4307e-02,  2.5996e-01,  4.2592e-01, -1.1922e-01,  1.3692e-01,\n",
       "          -3.2956e-01, -7.4432e-01, -2.5966e-01,  9.6324e-01,  7.7465e-01,\n",
       "           1.9670e-01, -1.9042e-02, -8.1636e-02, -1.9276e-01,  3.5361e-02,\n",
       "          -4.0494e-01,  4.8215e-01, -2.7105e-01,  1.7095e-01, -3.7705e-01,\n",
       "           1.0429e-01,  3.7197e-01,  3.4084e-01,  5.6575e-01, -1.4612e-01,\n",
       "           9.4235e-02,  2.8574e-01, -1.5445e-01,  1.5536e-01,  3.5582e-02,\n",
       "          -3.8031e-01, -8.8790e-02,  3.0893e-01, -6.9099e-02,  4.7056e-01,\n",
       "           4.9764e-01,  2.3762e-01,  6.5643e-02,  9.9503e-02, -8.3663e-01,\n",
       "          -2.5279e-01,  5.6585e-01,  2.1565e-01,  6.3207e-01, -1.1676e-01,\n",
       "           5.3984e-01, -4.4298e-01, -5.2594e-01,  1.1194e-01,  1.9656e-01,\n",
       "           7.3552e-02, -4.3098e-02,  2.0078e-01, -7.0383e-01, -8.4127e-02,\n",
       "           1.6778e-02, -8.7682e-01, -4.7889e-01,  5.1225e-01,  7.9790e-01,\n",
       "          -6.7119e-02,  2.6703e-01,  2.3978e-01, -5.9687e-01, -5.6741e-01,\n",
       "           2.7721e-01,  2.7889e-01,  1.8571e-01, -8.7278e-02, -3.2427e-02,\n",
       "           2.3645e-01, -5.8948e-01, -1.4127e-01, -7.9560e-01, -2.0029e-01,\n",
       "          -7.0919e-01,  6.2702e-02, -2.7566e-01,  1.4660e-01,  1.1762e-01,\n",
       "          -2.8113e-01, -3.7951e-02,  3.4275e-02,  1.6713e-01, -1.1871e-01,\n",
       "           1.8381e-01,  1.8482e-01, -3.3312e-01, -1.2294e-01,  7.5031e-02,\n",
       "           5.4269e-01,  4.6016e-01,  2.4860e-01,  2.2734e-02,  1.5203e-01,\n",
       "          -4.8605e-01,  3.4140e-01, -6.0558e-01, -4.3564e-01, -2.9054e-01,\n",
       "           1.3751e-01, -2.3677e-02,  2.0992e-02, -2.6183e-01, -1.1759e-02,\n",
       "           1.2478e-01,  2.3352e-01,  7.7764e-01, -1.7388e-01,  9.4571e-01,\n",
       "           3.4690e-01,  3.7091e-01, -5.5496e-02,  1.0299e-01,  8.9637e-01,\n",
       "          -2.7613e-01, -2.5889e-01, -7.1012e-01, -1.3169e-01,  2.2696e-01,\n",
       "          -2.7500e-01, -3.4179e-02, -3.6653e-01,  9.1316e-02, -4.8833e-03,\n",
       "          -5.6162e-01,  8.5005e-01,  5.5760e-01, -1.5316e-01,  3.2379e-01,\n",
       "           7.0232e-03,  2.0008e-01, -7.4017e-01, -4.3544e-01, -7.8054e-01,\n",
       "           4.2550e-01, -3.1170e-01,  3.0348e-01,  9.9859e-01, -5.3202e-01,\n",
       "          -6.3169e-01, -4.7653e-01,  5.5892e-02,  1.5069e-01,  2.9154e-01,\n",
       "          -5.9273e-01,  1.6392e-01, -6.0404e-02, -2.8780e-01, -2.0975e-01,\n",
       "           2.6104e-01, -2.9765e-01,  3.2060e-02, -1.7419e-01,  8.1086e-01,\n",
       "           1.0781e-01, -1.2152e-01,  2.4403e-01,  1.4212e-01,  1.1494e-02,\n",
       "          -1.0458e+00,  6.9771e-01, -2.4834e-01,  2.3887e-01, -3.5193e-01,\n",
       "           4.6563e-01, -3.4215e-01,  1.6948e-01,  5.5552e-02, -3.6116e-01,\n",
       "          -1.4375e-01,  9.7708e-02,  3.2745e-01, -5.5289e-01,  6.4486e-02,\n",
       "          -3.5076e-01,  3.3516e-01,  4.2541e-01,  2.8327e-01, -6.5185e-01,\n",
       "           3.0201e-01, -5.5735e-01,  3.6923e-01, -5.6051e-01,  3.8054e-01,\n",
       "          -3.8564e-01,  3.2565e-01, -7.9786e-01,  1.0987e-01,  4.4398e-02,\n",
       "          -7.4692e-01, -1.4689e-01, -6.2908e-01, -1.7448e-01,  8.2135e-02,\n",
       "           7.2187e-01,  2.7012e-02, -1.6823e-01, -2.4449e-02,  1.0402e+00,\n",
       "           5.0889e-01,  2.9972e-01, -1.6355e-01,  3.1577e-01, -8.8307e-01,\n",
       "          -2.0887e-02,  3.4785e-01,  3.0777e-01, -1.5368e-01, -5.3157e-01,\n",
       "          -2.0838e-01,  2.1587e-01,  3.3548e-02,  3.0211e-01,  7.9288e-02,\n",
       "          -7.9227e-01,  3.7398e-01,  7.2632e-02, -3.7864e-01,  3.1085e-01,\n",
       "          -6.4052e-02, -2.0371e-01,  1.1534e+00, -7.2804e-01,  8.2926e-01,\n",
       "          -7.9876e-01, -2.1094e-02,  7.5177e-01,  2.3525e-01,  4.2065e-02,\n",
       "          -1.5018e-01, -4.7689e-02,  5.2196e-01,  9.2577e-02, -4.4603e-02,\n",
       "           3.1897e-01, -2.3406e-01,  1.3497e-01,  8.5720e-01,  7.1635e-01,\n",
       "           1.3496e-01,  1.5300e-02, -4.3508e-01, -1.3270e-01, -6.2689e-01,\n",
       "           2.1203e-01,  3.2642e-01, -3.0152e-01,  3.6910e-01, -3.6612e-01,\n",
       "          -2.5846e-01, -1.2203e-01, -5.0751e-02, -1.5171e-01, -1.2795e-02,\n",
       "          -2.6310e-01,  3.9450e-01,  6.4255e-01,  4.1315e-01,  1.9948e-01,\n",
       "          -2.0295e-01,  8.1469e-02,  5.5644e-01, -1.7796e-01, -3.2732e-01,\n",
       "           8.0365e-02,  5.6137e-01, -2.9861e-01,  9.8971e-01,  3.4138e-01,\n",
       "           1.4840e-01,  3.8377e-01,  1.0319e-02,  3.0989e-01,  5.5286e-01,\n",
       "          -2.3905e-01,  2.2907e-01, -2.5986e-01, -3.3483e-01, -3.9521e-02,\n",
       "          -4.1949e-01, -6.2418e-01, -1.4899e-01,  7.9644e-02,  1.9520e-01,\n",
       "          -6.6927e-01,  2.9553e-01, -1.6182e-01,  2.6665e-01,  4.6309e-01,\n",
       "          -3.6727e-01, -6.9704e-01, -2.0258e-01, -1.6160e-01, -2.7270e-01,\n",
       "           7.1255e-01,  1.2592e-01,  1.2822e-02, -1.7181e-01,  5.8526e-01,\n",
       "          -6.7411e-01,  3.3048e-01, -1.0021e+00, -2.5921e-01, -7.6338e-03,\n",
       "           2.0836e-01, -3.9690e-01,  1.8674e-01, -8.5211e-01, -1.9347e-01,\n",
       "           4.3609e-01,  1.4635e-01, -6.4104e-01,  4.2491e-01,  2.4748e-01,\n",
       "          -3.9834e-01, -2.7697e-01, -4.2586e-02,  3.3936e-01, -3.7574e-01,\n",
       "          -1.7750e-01, -1.1325e-01,  2.7035e-01,  6.4995e-03,  9.8403e-01,\n",
       "          -6.5307e-01, -5.0104e-01, -1.7086e-01,  2.7777e-01, -2.9488e-01,\n",
       "          -3.1845e-01,  1.0489e+00,  3.9719e-01,  1.0477e-01,  3.6112e-01,\n",
       "           1.0937e-01, -2.0151e-01, -1.5881e-01, -1.1419e-01, -4.1140e-01,\n",
       "          -3.4205e-01,  7.0649e-03,  4.7111e-01, -1.1401e-01,  3.6992e-01,\n",
       "          -3.5886e-02, -1.3173e-01,  9.8094e-02,  3.6495e-01,  2.0337e-01,\n",
       "          -1.5654e-01, -3.3956e-01, -7.6999e-01,  1.3447e-01, -6.9593e-01,\n",
       "           5.4476e-01, -3.0508e-01, -1.1985e-01, -1.1642e-01,  2.8941e-01,\n",
       "          -9.7424e-01, -3.3696e-01,  5.9411e-01, -1.3237e-02,  2.8966e-01,\n",
       "           5.8729e-01, -1.0080e-01,  3.8075e-02, -5.1110e-01, -3.3084e-01,\n",
       "          -3.2253e-01,  1.2305e+00,  2.7379e-01, -8.1908e-03, -5.1911e-02,\n",
       "           2.5744e-01]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.1975e-02,  5.7746e-01, -1.9494e-01, -1.9142e-02,  6.5712e-01,\n",
       "           4.4960e-02, -1.1673e-01, -9.8820e-02,  1.4473e-01,  4.6904e-01,\n",
       "          -1.3011e-01, -7.3583e-01, -3.8341e-01,  6.2809e-01,  4.1460e-01,\n",
       "          -1.9454e-01,  5.9161e-01,  7.4418e-01, -2.4644e-01, -3.7124e-01,\n",
       "           7.6690e-01, -1.4048e-01,  5.0862e-01,  3.8526e-01,  6.1598e-01,\n",
       "           3.1927e-01, -2.2774e-02,  4.7335e-02,  2.5300e-01,  3.2634e-01,\n",
       "          -6.0402e-01,  3.4767e-01, -2.5557e-01, -1.9777e-01, -7.7682e-02,\n",
       "          -4.9658e-02, -2.8133e-01,  1.8684e-02, -2.9245e-01,  4.1414e-01,\n",
       "           2.1180e-03,  2.6046e-01, -2.6050e-01, -4.8846e-01, -2.5690e-02,\n",
       "          -2.9093e-02, -6.2434e-02, -5.3825e-01,  1.1888e-01, -9.8285e-02,\n",
       "          -4.4191e-01,  1.3444e-01,  4.0320e-01,  7.3316e-01, -5.7567e-01,\n",
       "           2.7818e-01, -3.0841e-01,  1.9521e-01,  2.2041e-01,  3.4297e-01,\n",
       "           1.5299e-01,  1.4999e-01,  8.5068e-02, -3.5624e-01,  1.2030e-01,\n",
       "          -2.5454e-01, -3.1050e-01,  3.0429e-01,  4.2177e-02, -1.0874e+00,\n",
       "          -1.0869e-01, -7.4825e-01, -1.7894e-01,  6.9001e-01,  4.6395e-01,\n",
       "          -3.4968e-01,  2.5866e-01, -4.7633e-01, -2.1349e-01,  4.7492e-01,\n",
       "          -2.2597e-02, -1.6454e-01,  2.7754e-02, -5.9524e-02,  4.0712e-01,\n",
       "          -2.7633e-01, -4.0022e-01, -7.6981e-03,  3.5381e-01, -2.1375e-01,\n",
       "          -1.1920e-01,  2.7936e-01, -6.8206e-01,  1.1023e-01, -6.6119e-01,\n",
       "           9.7619e-03,  1.4541e-01, -1.5481e-01, -9.1952e-01,  3.3034e-01,\n",
       "          -3.8018e-01, -8.9986e-01, -1.7744e-01,  3.2998e-01, -2.4112e-01,\n",
       "          -7.7792e-02, -5.4978e-01, -3.4560e-01,  6.5338e-02, -2.9056e-01,\n",
       "           2.5392e-01,  6.7200e-01,  7.4194e-02,  6.4344e-01, -6.8551e-01,\n",
       "          -5.0553e-01,  1.0065e+00, -7.4033e-02, -1.3023e-02, -9.6510e-02,\n",
       "           1.4576e-01,  2.7741e-02,  5.3825e-01,  2.0232e-01,  6.0783e-02,\n",
       "           9.4135e-02, -9.1531e-01, -6.1123e-01,  1.7499e-01,  3.3339e-01,\n",
       "           2.9972e-01, -2.0732e-01, -2.7278e-01,  7.3138e-02,  5.2838e-01,\n",
       "          -5.5883e-01,  1.0691e-01, -2.2955e-01,  3.0250e-01,  5.1500e-01,\n",
       "          -3.9082e-01,  2.1407e-01,  2.6965e-02, -1.3864e-01, -5.5958e-01,\n",
       "           7.7839e-02,  1.6306e-01, -3.0417e-01, -8.0368e-02, -4.1292e-01,\n",
       "          -4.6045e-01,  1.6345e-03, -1.2626e-01,  7.0734e-02,  6.3093e-01,\n",
       "          -7.8552e-02, -2.4516e-01, -2.2383e-01,  1.5611e-02,  1.3436e-01,\n",
       "          -5.0620e-01, -9.2264e-02,  9.4569e-02, -7.1714e-01,  3.0935e-01,\n",
       "           2.8034e-01,  7.5566e-02,  3.8740e-01,  5.3525e-02, -2.5170e-01,\n",
       "          -2.5109e-01, -7.5641e-01,  1.2487e-01, -1.7491e-02, -3.2212e-01,\n",
       "          -6.8759e-01,  1.5474e-01,  6.3887e-01, -1.8296e-01,  1.0192e+00,\n",
       "          -2.5318e-01, -4.4326e-01, -1.4298e-01, -2.7230e-01, -8.3584e-01,\n",
       "          -4.8944e-01, -8.7853e-01, -9.9357e-02,  3.8071e-01, -1.6379e-01,\n",
       "           2.2459e-01,  7.5682e-01, -2.8486e-01, -6.7951e-01, -4.8172e-01,\n",
       "          -1.0209e-01,  2.2079e-01, -3.2075e-01,  4.3176e-03, -7.2831e-01,\n",
       "           3.4095e-01,  3.7042e-01, -3.4836e-01, -3.3125e-01, -6.1600e-01,\n",
       "          -5.4651e-01,  7.1740e-01, -2.7718e-02,  9.9538e-02,  5.7503e-01,\n",
       "          -3.4859e-01,  2.8575e-02, -9.4642e-02,  5.9343e-01,  2.9759e-01,\n",
       "           5.1099e-01,  2.6657e-01,  2.3031e-01, -1.0420e+00,  4.5032e-01,\n",
       "          -1.4495e-01, -2.9440e-01, -3.2455e-01,  6.2209e-01, -3.5662e-02,\n",
       "           4.0438e-01,  2.2423e-01, -3.1268e-01,  4.2649e-01,  5.1531e-01,\n",
       "           1.7906e-01, -8.0632e-02,  4.5546e-01,  2.7833e-01,  1.1292e-01,\n",
       "          -3.6390e-01, -6.3863e-02,  4.4081e-01, -4.6165e-01, -3.1570e-01,\n",
       "          -9.2056e-01, -1.9186e-01, -9.0970e-01,  6.2688e-01,  8.5570e-02,\n",
       "           5.8645e-02,  1.2500e-02, -9.1694e-02, -2.0044e-01,  1.5284e-01,\n",
       "          -4.3519e-01, -3.0894e-01, -2.9999e-01, -1.0060e+00, -3.9749e-01,\n",
       "           4.0733e-01, -5.7776e-01,  2.9587e-01, -3.9518e-04, -4.6156e-01,\n",
       "           1.2341e-01,  3.1138e-01, -1.3809e-02,  1.9414e-02, -7.2843e-01,\n",
       "           4.9345e-01,  1.7448e-01,  7.1935e-01, -5.7264e-01, -2.5112e-02,\n",
       "          -3.0192e-01,  1.0003e-02, -2.1275e-01,  1.5706e-01,  1.2856e-02,\n",
       "          -2.7470e-01,  5.3209e-01, -3.2680e-01,  3.1471e-01,  1.4219e-01,\n",
       "          -3.8681e-02,  6.9663e-01, -5.6296e-01,  3.2346e-02, -2.3660e-01,\n",
       "          -4.2415e-01,  6.9882e-01,  2.6961e-01,  4.9223e-02,  3.7404e-01,\n",
       "           8.4278e-02,  3.7066e-01,  7.0707e-01, -4.4988e-01, -3.0312e-01,\n",
       "           2.7590e-01,  3.6319e-01,  2.2724e-01,  7.1499e-01, -9.1655e-01,\n",
       "           2.3484e-01, -1.6835e-01,  4.3868e-02,  1.6202e-01,  3.8860e-01,\n",
       "          -8.0440e-02,  5.0942e-02,  8.0525e-01,  8.4965e-03,  7.2190e-01,\n",
       "           6.3656e-01, -2.3533e-01,  2.5779e-01,  3.7272e-02,  4.0506e-01,\n",
       "          -1.6895e-01,  1.1692e+00, -2.3832e-01,  2.5405e-01,  4.7162e-01,\n",
       "           7.0964e-01, -6.2248e-01,  1.3764e-01, -2.1280e-01, -4.3907e-01,\n",
       "          -1.3131e-01,  8.5205e-01, -1.4492e-01,  1.4988e-01,  9.4383e-01,\n",
       "           3.9611e-01,  5.9641e-02,  5.0454e-01,  2.3660e-01, -4.3606e-01,\n",
       "           9.0535e-02,  5.6694e-01, -1.7757e-01, -7.8117e-01, -4.0334e-01,\n",
       "           1.2243e-01, -3.5574e-01, -6.1463e-01,  2.7067e-01, -4.9927e-01,\n",
       "           4.7269e-01,  3.3601e-02,  4.5294e-01,  2.6375e-02, -2.1882e-02,\n",
       "          -2.2798e-01,  5.0017e-01,  1.7535e-01, -1.9696e-01,  1.1115e-01,\n",
       "           5.4155e-01, -7.2330e-01,  1.3401e-01,  4.7294e-01, -3.2420e-01,\n",
       "           3.6396e-01,  4.5112e-01, -9.0843e-01, -3.3900e-01, -5.8901e-01,\n",
       "           4.7483e-02, -1.1649e+00, -3.6239e-01, -5.2235e-02,  9.1471e-01,\n",
       "          -2.4076e-01,  5.8639e-01,  1.0789e-01,  4.8828e-01,  9.6139e-02,\n",
       "           5.7027e-01,  4.0611e-01,  3.0979e-01,  1.8938e-01,  4.7151e-01,\n",
       "          -4.3684e-01,  6.0632e-02,  1.4877e-01,  5.5189e-01,  5.7546e-01,\n",
       "           6.2099e-01,  5.3287e-02, -3.2673e-01, -2.1498e-01,  7.3479e-01,\n",
       "          -1.3829e-01,  6.9752e-01,  7.3632e-01,  5.4853e-01,  2.3635e-01,\n",
       "           1.0038e+00, -4.5955e-02, -1.1738e-01, -3.7287e-02, -2.3980e-01,\n",
       "          -6.4432e-01,  3.2411e-01, -5.2175e-01,  3.0556e-01,  1.7844e-01,\n",
       "           3.0627e-01,  3.2107e-02,  2.3139e-01, -2.7415e-01, -1.4760e-01,\n",
       "          -2.4454e-01,  1.0898e-01, -4.1742e-01,  4.0058e-01, -5.7176e-01,\n",
       "           2.5430e-02,  1.0162e-03,  1.5844e-01, -3.3981e-02,  1.7150e-01,\n",
       "          -3.3204e-01,  5.2570e-01, -1.6045e-01,  6.3100e-02, -2.1683e-02,\n",
       "          -5.1368e-02,  3.8021e-01, -8.2935e-01, -5.8972e-01, -3.4339e-01,\n",
       "           8.0237e-02,  6.2421e-02, -3.2968e-01,  2.0846e-01, -9.5737e-03,\n",
       "          -3.4687e-01,  3.9252e-01, -2.8284e-02,  2.7076e-01,  4.8181e-01,\n",
       "          -4.1696e-02, -2.3137e-01, -6.4704e-01,  2.8452e-01, -4.2229e-02,\n",
       "          -7.2198e-02,  4.8925e-01, -1.6762e-01,  6.2444e-02,  1.9582e-01,\n",
       "           1.2918e-01,  3.0942e-01,  7.0467e-01,  7.7461e-02,  1.6958e-02,\n",
       "          -1.3681e-01, -6.0061e-02, -3.8704e-01,  4.5107e-01, -2.9831e-01,\n",
       "          -1.4659e-01, -4.8835e-01, -3.5654e-01,  3.3751e-01, -8.2118e-02,\n",
       "           3.7590e-02, -2.5949e-01,  4.5095e-02,  6.0341e-02,  5.7093e-01,\n",
       "           9.5470e-02,  4.1219e-01,  4.3815e-01,  1.4957e-01, -6.1866e-02,\n",
       "          -5.5572e-01,  1.1888e-01, -2.0760e-01, -5.7144e-01, -2.1319e-01,\n",
       "           4.6382e-01,  4.1595e-02,  4.0364e-01,  2.8656e-01, -4.1906e-01,\n",
       "           5.5538e-01,  2.8047e-01, -1.9714e-01,  5.5319e-01, -1.5813e-01,\n",
       "           5.9971e-01,  1.1753e-01, -2.0701e-01,  6.7423e-01, -1.1459e-01,\n",
       "          -6.0793e-01, -3.0690e-01,  3.5792e-01,  7.5568e-02, -3.2801e-01,\n",
       "           3.7901e-01, -2.1322e-01, -3.6692e-01,  3.0903e-01, -2.8635e-01,\n",
       "           2.5935e-01, -1.1789e-01, -3.8955e-01, -4.1409e-01, -1.5965e-01,\n",
       "          -4.7620e-01,  9.2309e-01,  1.6948e-01,  5.2329e-01,  4.9866e-01,\n",
       "          -5.3240e-01,  9.4485e-02,  3.1852e-01, -4.0229e-01,  4.7085e-02,\n",
       "           8.1236e-03,  5.0603e-01, -2.5622e-01,  5.0087e-01, -2.3668e-01,\n",
       "          -2.2450e-01, -7.0584e-02,  3.7810e-01,  4.2837e-01,  5.4477e-02,\n",
       "          -1.1313e-01, -1.9127e-01, -4.1020e-02,  1.3539e-01,  1.6747e-01,\n",
       "           2.2657e-01, -2.1985e-01, -6.7297e-01,  1.9847e-01,  5.4595e-01,\n",
       "          -3.4546e-01,  4.7591e-01, -7.3778e-01, -6.6361e-01,  2.2637e-01,\n",
       "          -6.3022e-01,  2.4959e-03,  5.7101e-02,  1.1036e-01,  9.0505e-02,\n",
       "          -7.3363e-02,  5.3445e-01, -1.4657e-01,  3.2230e-01, -6.0769e-01,\n",
       "          -1.8936e-01, -4.3759e-01,  3.6820e-01, -4.5126e-01, -5.6401e-02,\n",
       "          -2.4386e-01, -2.4306e-01,  5.0941e-01,  4.0340e-01, -2.8302e-01,\n",
       "          -3.5394e-01,  8.3253e-01,  3.8240e-01,  2.1181e-01, -3.6051e-01,\n",
       "          -7.9611e-02, -3.7568e-01,  2.3179e-03,  3.1484e-01,  7.0807e-01,\n",
       "           1.0781e-01, -6.6562e-02, -6.9895e-01, -3.8012e-01,  3.4867e-01,\n",
       "           3.6106e-01,  3.0367e-01,  9.5063e-02, -1.2392e-01, -8.1053e-02,\n",
       "          -1.1773e-02,  3.7853e-01,  2.4716e-01,  6.8970e-02,  7.0805e-01,\n",
       "           8.1400e-01,  2.4766e-01, -4.2658e-01,  1.4066e-01, -6.5078e-01,\n",
       "          -3.1779e-01, -4.6137e-01, -4.1810e-01,  3.7755e-02,  5.9122e-01,\n",
       "          -1.3423e-01,  3.2538e-01, -5.0544e-01, -1.1311e-01,  1.7989e-01,\n",
       "           5.3493e-01, -8.3053e-01, -2.7166e-02, -4.0321e-01,  6.0830e-02,\n",
       "          -9.7172e-02, -4.6446e-02,  3.9594e-01,  6.0036e-01, -4.0065e-01,\n",
       "           2.6692e-03,  4.6841e-01,  8.9267e-02,  1.5659e-01, -1.3713e-01,\n",
       "          -2.7935e-01,  6.9447e-02,  7.0525e-01, -5.3038e-02,  7.8611e-01,\n",
       "          -3.0652e-02, -7.2654e-01,  5.0458e-01,  7.0940e-01,  4.2880e-01,\n",
       "          -2.2257e-01,  1.1110e-01,  1.6651e-01, -5.5531e-02,  3.9942e-02,\n",
       "           1.4003e-01,  1.2624e-01,  1.1145e-01,  5.0052e-01, -6.3132e-01,\n",
       "           6.0551e-01,  1.7263e-01, -7.2414e-01, -3.1330e-01,  7.6101e-02,\n",
       "           5.8321e-01,  1.2724e-01,  7.7957e-01,  6.1630e-01, -7.6628e-01,\n",
       "           3.9539e-02, -2.7790e-01,  3.4705e-01,  1.1955e+00, -1.0050e-01,\n",
       "          -3.1953e-02, -2.4949e-01,  5.6854e-01,  4.0821e-02,  3.2058e-01,\n",
       "           7.8103e-02,  1.4440e-01, -5.2911e-01, -8.9598e-01,  1.8298e-01,\n",
       "          -1.8902e-01, -9.3330e-03,  8.9574e-03, -1.7593e-01,  2.0635e-04,\n",
       "           5.8454e-01, -3.0236e-01, -1.6007e-01, -2.2578e-01,  1.1712e-01,\n",
       "          -4.2534e-01, -3.6744e-01, -7.6557e-02, -4.0110e-01,  5.4476e-01,\n",
       "          -1.3669e-01, -4.4886e-01, -3.6466e-01, -8.9959e-01, -2.0423e-01,\n",
       "           8.0809e-01,  2.3360e-01, -4.9187e-01, -2.3726e-01, -2.2792e-01,\n",
       "           7.7431e-01,  2.1871e-01,  8.8178e-01, -6.6337e-01, -7.0774e-01,\n",
       "           2.6265e-02, -2.9645e-01, -1.7357e-01,  2.5203e-01, -2.3579e-01,\n",
       "          -2.4166e-01,  5.6271e-01, -1.1449e-01,  3.0481e-01, -3.7205e-01,\n",
       "          -4.8451e-01, -1.0971e+00,  5.2071e-01,  2.4254e-02,  1.0801e-02,\n",
       "           1.6840e-01,  5.3109e-01,  1.7875e-01,  5.2920e-01,  9.1081e-02,\n",
       "          -8.0441e-01, -6.2619e-01,  2.1260e-01,  4.1732e-01,  1.5698e-01,\n",
       "           2.4655e-01, -1.1691e-01, -1.3168e-01, -3.8575e-01, -1.8935e-02,\n",
       "          -1.7116e-02,  2.6504e-01, -1.0643e-01,  1.8471e-01, -2.6316e-01,\n",
       "          -3.5040e-01,  3.9638e-01, -4.7722e-01,  4.0853e-01, -5.3385e-01,\n",
       "          -2.8935e-01, -1.2063e-02,  4.5657e-01,  3.7491e-02, -1.7877e-01,\n",
       "          -8.6697e-01,  5.5544e-01, -2.8515e-01, -2.0513e-01,  1.2148e-01,\n",
       "           9.0727e-02, -1.2670e-01,  3.4481e-01, -2.2994e-01,  9.9539e-01,\n",
       "           7.2698e-02]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fliphor(inputs):\n",
    "    inv_idx = torch.arange(inputs.size(3)-1,-1,-1).long()  # N x C x H x W\n",
    "        # torch.save(self.model.state_dict(), \"./model_4.pth\")\n",
    "    return inputs.index_select(3,inv_idx)\n",
    "\n",
    "def extract_feature(model,input):\n",
    "        features = torch.FloatTensor()\n",
    "        # for (inputs, labels) in loader:\n",
    "        ff = torch.FloatTensor(input.size(0), 2048).zero_()\n",
    "        for i in range(2):\n",
    "            if i==1:\n",
    "                input = fliphor(input)\n",
    "            # input_img = inputs.to(self.device)\n",
    "            outputs = model(input)\n",
    "            f = outputs[0].data.cpu()\n",
    "            ff = ff + f\n",
    "            # print(len(outputs))\n",
    "        fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "        \n",
    "        ff = ff.div(fnorm.expand_as(ff))\n",
    "\n",
    "        features = torch.cat((features, ff), 0)\n",
    "        # torch.save(self.model.state_dict(), \"./model_5.pth\")\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "e_f = extract_feature(model,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.common import list_pictures\n",
    "\n",
    "from torch.utils.data import dataset\n",
    "from torchvision.datasets.folder import default_loader\n",
    "class Market1501(dataset.Dataset):\n",
    "    def __init__(self, args, transform, dtype):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.loader = default_loader\n",
    "\n",
    "        data_path = args.datadir\n",
    "        if dtype == 'train':\n",
    "            data_path += '/bounding_box_train'\n",
    "        elif dtype == 'test':\n",
    "            data_path += '/bounding_box_test'\n",
    "        else:\n",
    "            data_path += '/query'\n",
    "\n",
    "        \n",
    "        self.imgs = [path for path in list_pictures(data_path) if self.id(path) != -1]\n",
    "\n",
    "        self._id2label = {_id: idx for idx, _id in enumerate(self.unique_ids)}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.imgs[index]\n",
    "        target = self._id2label[self.id(path)]\n",
    "\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_list = os.listdir(root_dir)\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((384,128), interpolation=3),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.file_list[idx])\n",
    "        image = Image.open(img_name)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract label from the first 4 characters of the image name\n",
    "        label = int(img_name.split(os.path.sep)[-1][:4])\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the dataset\n",
    "dataset_path = \"/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/data/Market-1501-v15.09.15/test\"\n",
    "\n",
    "# Create a CustomDataset instance\n",
    "custom_dataset = CustomDataset(root_dir=dataset_path)\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "batch_size = 4\n",
    "data_loader = DataLoader(dataset=custom_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty lists to store the embeddings and labels\n",
    "embedding_list = []\n",
    "label_list = []\n",
    "\n",
    "# Iterate over the batches of data\n",
    "for batch_images, batch_labels in data_loader:\n",
    "    # Move the batch of images to the device (GPU if available)\n",
    "    # batch_images = batch_images.to(device)\n",
    "    \n",
    "    # Pass the batch of images through the model\n",
    "\n",
    "    output = extract_feature(model,batch_images)\n",
    "    # output = torch.tensor(output)\n",
    "    output = list(output)\n",
    "    # print(\"output\", output)\n",
    "    # Split the output embeddings for each image in the batch\n",
    "    # embeddings = torch.split(output, 1)\n",
    "    \n",
    "    # Iterate over the embeddings and labels in the batch\n",
    "    for embedding, label in zip(output, batch_labels):\n",
    "        # Convert the embedding tensor to a list of numbers\n",
    "        embedding_list.append(embedding.squeeze().cpu().detach().numpy().tolist())\n",
    "        label_list.append(label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "data = [embedding + [label] for embedding, label in zip(embedding_list, label_list)]\n",
    "print(len(data))\n",
    "# Define column names\n",
    "columns = [f\"feature_{i}\" for i in range(len(embedding_list[0]))] + [\"Label\"]\n",
    "\n",
    "# Create DataFrame from the list of lists\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the dataset\n",
    "dataset_path2 = \"/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/data/Market-1501-v15.09.15/test\"\n",
    "\n",
    "# Create a CustomDataset instance\n",
    "custom_dataset2 = CustomDataset(root_dir=dataset_path2)\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "batch_size = 4\n",
    "data_loader2 = DataLoader(dataset=custom_dataset2, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty lists to store the embeddings and labels\n",
    "embedding_list2 = []\n",
    "label_list2 = []\n",
    "\n",
    "# Iterate over the batches of data\n",
    "for batch_images, batch_labels in data_loader2:\n",
    "    # Move the batch of images to the device (GPU if available)\n",
    "    # batch_images = batch_images.to(device)\n",
    "    \n",
    "    # Pass the batch of images through the model\n",
    "    output = extract_feature(model,batch_images)\n",
    "    embeddings = list(output)\n",
    "    # Split the output embeddings for each image in the batch\n",
    "    # embeddings = torch.split(output, 1)\n",
    "    \n",
    "    # Iterate over the embeddings and labels in the batch\n",
    "    for embedding, label in zip(embeddings, batch_labels):\n",
    "        # Convert the embedding tensor to a list of numbers\n",
    "        embedding_list2.append(embedding.squeeze().cpu().detach().numpy().tolist())\n",
    "        label_list2.append(label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine embeddings and labels into a list of lists\n",
    "data2 = [embedding + [label] for embedding, label in zip(embedding_list2, label_list2)]\n",
    "\n",
    "# Define column names\n",
    "columns2 = [f\"feature_{i}\" for i in range(len(embedding_list2[0]))] + [\"Label\"]\n",
    "\n",
    "# Create DataFrame from the list of lists\n",
    "df2 = pd.DataFrame(data2, columns=columns2)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df2.to_csv('/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/embeddings_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0   0.024536   0.010658   0.068860        0.0   0.005983   0.040559   \n",
      "1   0.018952   0.000385   0.047369        0.0   0.001945   0.006917   \n",
      "2   0.000000   0.000000   0.056089        0.0   0.027257   0.029233   \n",
      "3   0.041531   0.000000   0.042104        0.0   0.033183   0.040782   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_2039  \\\n",
      "0   0.043827   0.000000   0.058975        0.0  ...      0.000000   \n",
      "1   0.021748   0.000000   0.034938        0.0  ...      0.004057   \n",
      "2   0.028987   0.000000   0.031728        0.0  ...      0.004930   \n",
      "3   0.053861   0.031068   0.013767        0.0  ...      0.047855   \n",
      "\n",
      "   feature_2040  feature_2041  feature_2042  feature_2043  feature_2044  \\\n",
      "0           0.0      0.000000      0.007931      0.010375           0.0   \n",
      "1           0.0      0.000863      0.000375      0.022104           0.0   \n",
      "2           0.0      0.012133      0.000000      0.029638           0.0   \n",
      "3           0.0      0.000000      0.001251      0.058719           0.0   \n",
      "\n",
      "   feature_2045  feature_2046  feature_2047  Label  \n",
      "0      0.023951           0.0      0.026080      2  \n",
      "1      0.021950           0.0      0.010123      2  \n",
      "2      0.028207           0.0      0.000000      2  \n",
      "3      0.000000           0.0      0.037997     23  \n",
      "\n",
      "[4 rows x 2049 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/embeddings.csv')\n",
    "\n",
    "# Split the DataFrame into features (embeddings) and labels\n",
    "X_train = df.iloc[:, :-1]\n",
    "y_train = df.iloc[:, -1]\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "print(df)\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linhphuong/anaconda3/envs/id/lib/python3.9/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df_test = pd.read_csv('/home/linhphuong/Documents/documents/persion_re_id/MGN-pytorch-modify/embeddings_test.csv')\n",
    "\n",
    "# Split the DataFrame into features (embeddings) and labels\n",
    "X_test = df_test.iloc[:, :-1]\n",
    "y_test = df_test.iloc[:, -1]\n",
    "\n",
    "# Ensure that X_test has the same format as X_train\n",
    "# If X_train has more features, you can adjust X_test accordingly\n",
    "\n",
    "# Use the trained KNN classifier to predict the labels for the test data\n",
    "y_pred = knn.predict(X_test.values)\n",
    "\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "id",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
